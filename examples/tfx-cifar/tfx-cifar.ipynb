{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydsPE-aN1Alm"
   },
   "source": [
    "##### Copyright &copy; 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCSByGH6C7zS"
   },
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TyrY7lV0oke"
   },
   "source": [
    "# Continuous training pipeline for CIFAR10 image classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLYriYe10okf"
   },
   "source": [
    "This example demonstrates a continuous training TFX pipeline that trains an image classification model on the CIFAR10 dataset. The pipeline runs on **AI Platform Pipelines** and uses **Cloud Dataflow** and **Cloud AI Platform Training** as execution runtimes.\n",
    "\n",
    "![TFX CAIP](../../images/tfx-caip-1.png)\n",
    "\n",
    "This example uses the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset released by the The Canadian Institute for Advanced Research (CIFAR).\n",
    "\n",
    "Note: This site provides applications using data that has been modified for use from its original source, The Canadian Institute for Advanced Research (CIFAR). The Canadian Institute for Advanced Research (CIFAR) makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at oneâ€™s own risk.\n",
    "\n",
    "You can read more about the dataset in [CIFAR dataset homepage](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxPMeugQ0okg"
   },
   "source": [
    "### Install KFP and TFX SDKs.\n",
    "\n",
    "You will use TFX and KFP SDKs to compile, deploy and run the pipeline. During the installation you may see errors like the one below.\n",
    "\n",
    ">\"ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\" \n",
    "\n",
    "Please ignore these errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_file = 'requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {requirements_file}\n",
    "\n",
    "tfx==0.21.0\n",
    "kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNiqq_kN0okj"
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade  -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX CLI requires [skaffold](https://skaffold.dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `PATH` to include user python binary directory and a directory containing `skaffold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43ncix2Q0okm"
   },
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX1rqpbQ0okp"
   },
   "source": [
    "Double-check the version of TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAIoKMNG0okq"
   },
   "outputs": [],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7JLpaXT0okv"
   },
   "source": [
    "### Configure AI Platform Pipelines connection settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to update the following constants with your settings:\n",
    "- Set `GCP_PROJECT` to your project ID\n",
    "- Set `ENDPOINT` to the endpoint of your AI Platform Pipelines environment\n",
    "\n",
    "The endpoint of the AI Platform Pipelines environment can be found in [AI Platform Pipelines Console](https://console.cloud.google.com/ai-platform/pipelines/clusters). \n",
    "1. Open the *SETTINGS* for your instance. \n",
    "2. Use the value of the host variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SDK* section of the pop-up window.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw3nsooU0okv"
   },
   "outputs": [],
   "source": [
    "GCP_PROJECT_ID='mlops-workshop'\n",
    "ENDPOINT='b408c7cc27aa8bb-dot-us-central2.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket to be used as an artifact store\n",
    "\n",
    "As the pipeline executes it stores generated artifacts in a GCS location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_STORE_URI='gs://{}-artifact-store'.format(GCP_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb {ARTIFACT_STORE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the compute region for Dataflow and AI Platform Training and Prediction\n",
    "The pipeline uses **Cloud Dataflow** and **Cloud AI Platform Training and Prediction** as execution runtimes for TFX components. In this example, the `us-central1` region is used as the default region for these services. Update the `GCP_REGION` constant if you cannot use `us-central1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6T-KXeA0ok3"
   },
   "source": [
    "### Set the URI for the custom TFX image\n",
    "\n",
    "The pipeline components execute in a runtime provided by a custom docker image. The image is a derivate of a base TFX image from Docker Hub - `tensorflow/tfx:0.21.x`. The custom image updates the base image with the latest TFX libraries and adds Python modules for `Transform` and `Train` components. The image will be built and pushed to your project's **Container Registry** by TFX CLI.\n",
    "\n",
    "The custom image is defined in the Dockerfile that can be found in the root folder of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ztxXOVD0ok4"
   },
   "outputs": [],
   "source": [
    "CUSTOM_TFX_IMAGE='gcr.io/' + GCP_PROJECT_ID + '/cifar-tfx-image'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cxlbi1QM0ok8"
   },
   "source": [
    "## Understanding the pipeline design\n",
    "\n",
    "The pipeline code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` file collates all configuration settings that are environment specific and sets the default values for the settings. The default values can be overwritten when building the pipeline by providing new values in a set of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 15 pipeline/config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file contains the core DSL defining the workflow implemented by the pipeline. \n",
    "\n",
    "The `transform_train.py` file contains data preprocessing and training code for the `Transform` and `Train` components.\n",
    "\n",
    "The `runner.py` file contains the code that configures and executes `KubeflowDagRunner`. `KubeflowDagRunner` is responsible for compiling the pipeline's DSL into the pipeline package (in the [argo](https://argoproj.github.io/argo/) format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the compile settings\n",
    "\n",
    "As noted the default values for the environment specific settings that control how the pipeline is compiled can be overwritten by the values in a set of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='cifar10-classifier'\n",
    "PIPELINE_NAME='cifar10_continuous_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIPlt-700ok-"
   },
   "outputs": [],
   "source": [
    "%env PROJECT_ID={GCP_PROJECT_ID}\n",
    "%env KUBEFLOW_TFX_IMAGE={CUSTOM_TFX_IMAGE}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env PIPELINE_NAME={PIPELINE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can build and upload the pipeline to the KFP environment in one step, using the `tfx pipeline create` command. The `tfx pipeline create` goes through the following steps:\n",
    "- (Optional) Builds an image to host your components, \n",
    "- Compiles the pipeline DSL into a pipeline package \n",
    "- Uploads the pipeline package to the KFP environment.\n",
    "\n",
    "As you are debugging the pipeline DSL, you may prefer to first use the `tfx pipeline compile` command, which only executes the compilation step. After the DSL compiles successfully you can use `tfx pipeline create` to go through all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path pipeline/runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozHIomcd0olB"
   },
   "source": [
    "### Deploy the pipeline to AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxOT19QS0olH"
   },
   "source": [
    "After the pipeline code has been debbuged and compiles without any errors you perform the final compilation and deploy the pipeline package in one step using the `tfx pipeline create` command. This command also builds the container image that hosts TFX components and your data preprocessing and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOU7zQof0olf"
   },
   "outputs": [],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline_path=pipeline/runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and monitor a pipeline run\n",
    "After the pipeline has been deployed, you can trigger and monitor pipeline runs using TFX CLI or KFP UI.\n",
    "\n",
    "To submit the pipeline run using TFX CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKSjVVsa0oli"
   },
   "outputs": [],
   "source": [
    "!tfx run create --pipeline_name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg0VxvUC0olk"
   },
   "source": [
    "To list all active runs of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run list --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID=[YOUR RUN ID]\n",
    "\n",
    "!tfx run status --pipeline_name {PIPELINE_NAME} --run_id {RUN_ID} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To terminate the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tfx run terminate --run_id [YOUR_RUN_ID] --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20KRGsPX0ol3"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Alternatively, you can clean up individual resources by visiting each consoles:\n",
    "- [Google Cloud Storage](https://console.cloud.google.com/storage)\n",
    "- [Google Container Registry](https://console.cloud.google.com/gcr)\n",
    "- [Google Kubernetes Engine](https://console.cloud.google.com/kubernetes)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
