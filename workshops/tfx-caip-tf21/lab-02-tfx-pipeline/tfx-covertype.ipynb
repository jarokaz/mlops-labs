{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/envs/tfx/bin:/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing TFX CLI.\n",
    "#PATH=%env PATH\n",
    "#%env PATH=/opt/conda/envs/tfx/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab walks you through a TFX pipeline that uses **Cloud Dataflow** and **Cloud AI Platform Training** as execution runtimes. The pipeline implements a typical TFX workflow as depicted on the below diagram:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n",
    "The pipeline source code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 4 root root 4096 Mar 16 20:17 .\n",
      "drwxr-xr-x 5 root root 4096 Mar 13 18:11 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar 13 17:58 .ipynb_checkpoints\n",
      "drwxr-xr-x 2 root root 4096 Mar 13 17:39 __pycache__\n",
      "-rw-r--r-- 1 root root 1323 Mar 13 15:42 config.py\n",
      "-rw-r--r-- 1 root root 6169 Mar 16 20:17 pipeline.py\n",
      "-rw-r--r-- 1 root root 2982 Mar 13 15:42 runner.py\n",
      "-rw-r--r-- 1 root root 8272 Mar 13 15:42 transform_train.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` module configures the default values for the environment specific settings and the default values for the pipeline runtime parameters. \n",
    "The default values can be overwritten at compile time by providing the updated values in a set of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Config:\n",
      "    \"\"\"Sets configuration vars.\"\"\"\n",
      "    \n",
      "    PIPELINE_NAME=os.getenv(\"PIPELINE_NAME\", \"cifar10_continuous_training\")\n",
      "    MODEL_NAME=os.getenv(\"MODEL_NAME\", \"covertype_classifier\")\n",
      "    PROJECT_ID=os.getenv(\"PROJECT_ID\", \"mlops-workshop\")\n",
      "    GCP_REGION=os.getenv(\"GCP_REGION\", \"us-central1\")\n",
      "    TFX_IMAGE=os.getenv(\"KUBEFLOW_TFX_IMAGE\", \"tensorflow/tfx:0.21.0\")\n",
      "    DATA_ROOT_URI=os.getenv(\"DATA_ROOT_URI\", \"gs://workshop-datasets/covertype/small\")\n",
      "    ARTIFACT_STORE_URI=os.getenv(\"ARTIFACT_STORE_URI\", \"gs://mlops-workshop-artifact-store\")\n",
      "    RUNTIME_VERSION=os.getenv(\"RUNTIME_VERSION\", \"2.1\")\n",
      "    PYTHON_VERSION=os.getenv(\"PYTHON_VERSION\", \"3.7\")\n",
      "    \n",
      "    \n",
      "    "
     ]
    }
   ],
   "source": [
    "!tail -n 15 pipeline/config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `transform_train.py` module implements data preprocessing and training logic for the `Transform` and `Train` components.\n",
    "\n",
    "The `runner.py` module configures and executes `KubeflowDagRunner`. At compile time, the `KubeflowDagRunner.run()` method conversts the TFX DSL into the pipeline package in the [argo](https://argoproj.github.io/argo/) format.\n",
    "\n",
    "The TFX components defined in the pipeline execute in a runtime provided by a custom docker image, which is a derivate of the base TFX image from Docker Hub - tensorflow/tfx:0.21.x. The custom image updates the base image with the latest TFX libraries and embeds the Python modules for `Transform` and `Train` components and the `schema` file defining input data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM tensorflow/tfx:0.21.2\n",
      "RUN python -m pip install -U tensorflow_model_analysis==0.21.6 tensorflow_data_validation==0.21.5 tensorflow-transform==0.21.2\n",
      "WORKDIR /pipeline\n",
      "COPY pipeline/transform_train.py ./\n",
      "COPY schema/schema.pbtxt ./schema/\n",
      "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As explained in the previous section, the environment specific settings can be provided through a set of environment variables and embedded into the pipeline package at compile time.\n",
    "\n",
    "### Configure environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `GCP_REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_REGION = 'us-central1'\n",
    "ENDPOINT = '6bec3f41fb14cbd3-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://hostedkfp-default-xmwajpzcy5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "DATA_ROOT_URI = 'gs://workshop-datasets/covertype/small'\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/{}/covertype-tfx-image'.format(PROJECT_ID)\n",
    "PIPELINE_NAME = 'tfx_covertype_continuous_training'\n",
    "MODEL_NAME = 'tf_covertype_classifier'\n",
    "RUNTIME_VERSION = '2.1'\n",
    "PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=mlops-dev-100\n",
      "env: KUBEFLOW_TFX_IMAGE=gcr.io/mlops-dev-100/covertype-tfx-image\n",
      "env: ARTIFACT_STORE_URI=gs://hostedkfp-default-xmwajpzcy5\n",
      "env: DATA_ROOT_URI=gs://workshop-datasets/covertype/small\n",
      "env: GCP_REGION=us-central1\n",
      "env: MODEL_NAME=tf_covertype_classifier\n",
      "env: PIPELINE_NAME=tfx_covertype_continuous_training\n",
      "env: RUNTIME_VERSION=2.1\n",
      "env: PYTHON_VERIONS=3.7\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env KUBEFLOW_TFX_IMAGE={CUSTOM_TFX_IMAGE}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERIONS={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can build and upload the pipeline to the AI Platform Pipelines instance in one step, using the `tfx pipeline create` command. The `tfx pipeline create` goes through the following steps:\n",
    "- (Optional) Builds the custom image to host your components, \n",
    "- Compiles the pipeline DSL into a pipeline package \n",
    "- Uploads the pipeline package to the instance.\n",
    "\n",
    "As you debug the pipeline DSL, you may prefer to first use the `tfx pipeline compile` command, which only executes the compilation step. After the DSL compiles successfully you can use `tfx pipeline create` to go through all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "/opt/conda/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:absl:feature_slicing_spec is deprecated, please use eval_config instead.\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/mlops-labs/workshops/tfx-caip-tf21/lab-02-tfx-pipeline/tfx_covertype_continuous_training.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path pipeline/runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package to AI Platform Pipelines\n",
    "\n",
    "After the pipeline code compiles without any errors you can use the `tfx pipeline create` command to perform the full build and deploy the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Skaffold configuration\n",
    "\n",
    "The `tfx pipeline create` command uses [Skafflod](https://skaffold.dev/) to build the custom TFX image. Before executing the tfx pipeline create command, modify the image field of the build.yaml file so it references your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sed: -e expression #1, char 2: unknown command: `S'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"SED_SCRIPT='s/\\\\([[:blank:]]*image:[[:blank:]]*\\\\).*/\\\\1gcr\\\\.io\\\\/'$PROJECT_ID'\\\\/custom_tfx/'\\nsed -i {SED_SCRIPT} build.yaml\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-142a239ba176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SED_SCRIPT='s/\\\\([[:blank:]]*image:[[:blank:]]*\\\\).*/\\\\1gcr\\\\.io\\\\/'$PROJECT_ID'\\\\/custom_tfx/'\\nsed -i {SED_SCRIPT} build.yaml\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"SED_SCRIPT='s/\\\\([[:blank:]]*image:[[:blank:]]*\\\\).*/\\\\1gcr\\\\.io\\\\/'$PROJECT_ID'\\\\/custom_tfx/'\\nsed -i {SED_SCRIPT} build.yaml\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "SED_SCRIPT='s/\\([[:blank:]]*image:[[:blank:]]*\\).*/\\1gcr\\.io\\/'$PROJECT_ID'\\/custom_tfx/'\n",
    "sed -i {SED_SCRIPT} build.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and upload the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/mlops-dev-100/covertype-tfx-image is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "Use skaffold to build the container image.\n",
      "/usr/local/bin/skaffold\n",
      "\u001b[33mWARN\u001b[0m[0348] error adding artifacts to cache; caching may not work as expected: inspecting image: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "/opt/conda/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:absl:feature_slicing_spec is deprecated, please use eval_config instead.\n",
      "WARNING:tensorflow:From /home/mlops-labs/workshops/tfx-caip-tf21/lab-02-tfx-pipeline/pipeline/pipeline.py:116: ModelValidator.__init__ (from tfx.components.model_validator.component) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "ModelValidator is deprecated, use Evaluator instead.\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/mlops-labs/workshops/tfx-caip-tf21/lab-02-tfx-pipeline/tfx_covertype_continuous_training.tar.gz\n",
      "{'created_at': datetime.datetime(2020, 3, 13, 18, 8, 41, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2020, 3, 13, 18, 8, 41, tzinfo=tzlocal()),\n",
      "                     'id': 'd54af158-1667-4da8-abc6-c8d974f9e811',\n",
      "                     'name': 'tfx_covertype_continuous_training',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'pipeline-root',\n",
      "                                     'value': 'gs://mlops-dev-100-artifact-store/tfx_covertype_continuous_training/{{workflow.uid}}'},\n",
      "                                    {'name': 'data-root-uri',\n",
      "                                     'value': 'gs://workshop-datasets/covertype/small'},\n",
      "                                    {'name': 'eval-steps', 'value': '500'},\n",
      "                                    {'name': 'train-steps', 'value': '500'}],\n",
      "                     'resource_references': [{'key': {'id': 'd54af158-1667-4da8-abc6-c8d974f9e811',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': 'd54af158-1667-4da8-abc6-c8d974f9e811',\n",
      " 'name': 'tfx_covertype_continuous_training',\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://mlops-dev-100-artifact-store/tfx_covertype_continuous_training/{{workflow.uid}}'},\n",
      "                {'name': 'data-root-uri',\n",
      "                 'value': 'gs://workshop-datasets/covertype/small'},\n",
      "                {'name': 'eval-steps', 'value': '500'},\n",
      "                {'name': 'train-steps', 'value': '500'}],\n",
      " 'url': None}\n",
      "Pipeline \"tfx_covertype_continuous_training\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline_path=pipeline/runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Deleting pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline tfx_covertype_continuous_training deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and monitor a pipeline run\n",
    "After the pipeline has been deployed, you can trigger and monitor pipeline runs using TFX CLI or KFP UI.\n",
    "\n",
    "To submit the pipeline run using TFX CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: tfx_covertype_continuous_training\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: tfx_covertype_continuous_training\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+\n",
      "| pipeline_name                     | run_id                               | status   | created_at                |\n",
      "+===================================+======================================+==========+===========================+\n",
      "| tfx_covertype_continuous_training | f0d1aec6-8fd2-45e9-b19f-74d459a192a6 |          | 2020-03-13T18:09:36+00:00 |\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline_name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all active runs of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run list --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID='[YOUR RUN ID]'\n",
    "\n",
    "!tfx run status --pipeline_name {PIPELINE_NAME} --run_id {RUN_ID} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
