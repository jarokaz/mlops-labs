{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Set `PATH` to include the directory containing TFX CLI.\n",
    "PATH=%env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab walks you through a TFX pipeline that uses **Cloud Dataflow** and **Cloud AI Platform Training** as execution runtimes. The pipeline implements a typical TFX workflow as depicted on the below diagram:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n",
    "The pipeline source code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\n",
      "drwxr-xr-x 4 jupyter jupyter 4096 Mar 18 21:46 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4096 Mar 18 21:46 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 2150 Mar 18 16:47 beam_runner.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1325 Mar 18 01:30 config.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1222 Mar 18 03:35 features.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Mar 18 21:44 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 jupyter jupyter 6565 Mar 18 21:29 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 7251 Mar 18 21:46 pipeline.py\n",
      "-rw-r--r-- 1 jupyter jupyter 2036 Mar 18 03:39 preprocessing.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Mar 18 18:03 __pycache__\n",
      "-rw-r--r-- 1 jupyter jupyter 3018 Mar 18 21:46 runner.py\n",
      "-rw-r--r-- 1 jupyter jupyter 7576 Mar 18 18:03 test.py\n",
      "-rw-r--r-- 1 jupyter jupyter 8272 Mar 18 01:30 transform_train.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` module configures the default values for the environment specific settings and the default values for the pipeline runtime parameters. \n",
    "The default values can be overwritten at compile time by providing the updated values in a set of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Config:\n",
      "    \"\"\"Sets configuration vars.\"\"\"\n",
      "    \n",
      "    PIPELINE_NAME=os.getenv(\"PIPELINE_NAME\", \"covertype_continuous_training\")\n",
      "    MODEL_NAME=os.getenv(\"MODEL_NAME\", \"covertype_classifier\")\n",
      "    PROJECT_ID=os.getenv(\"PROJECT_ID\", \"mlops-workshop\")\n",
      "    GCP_REGION=os.getenv(\"GCP_REGION\", \"us-central1\")\n",
      "    TFX_IMAGE=os.getenv(\"KUBEFLOW_TFX_IMAGE\", \"tensorflow/tfx:0.21.2\")\n",
      "    DATA_ROOT_URI=os.getenv(\"DATA_ROOT_URI\", \"gs://workshop-datasets/covertype/small\")\n",
      "    ARTIFACT_STORE_URI=os.getenv(\"ARTIFACT_STORE_URI\", \"gs://mlops-workshop-artifact-store\")\n",
      "    RUNTIME_VERSION=os.getenv(\"RUNTIME_VERSION\", \"2.1\")\n",
      "    PYTHON_VERSION=os.getenv(\"PYTHON_VERSION\", \"3.7\")\n",
      "    \n",
      "    \n",
      "    "
     ]
    }
   ],
   "source": [
    "!tail -n 15 pipeline/config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `transform_train.py` module implements data preprocessing and training logic for the `Transform` and `Train` components.\n",
    "\n",
    "The `runner.py` module configures and executes `KubeflowDagRunner`. At compile time, the `KubeflowDagRunner.run()` method conversts the TFX DSL into the pipeline package in the [argo](https://argoproj.github.io/argo/) format.\n",
    "\n",
    "The TFX components defined in the pipeline execute in a runtime provided by a custom docker image, which is a derivate of the base TFX image from Docker Hub - tensorflow/tfx:0.21.x. The custom image updates the base image with the latest TFX libraries and embeds the Python modules for `Transform` and `Train` components and the `schema` file defining input data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM tensorflow/tfx:0.21.2\n",
      "WORKDIR /pipeline\n",
      "COPY pipeline/* ./\n",
      "COPY schema/schema.pbtxt ./schema/\n",
      "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As explained in the previous section, the environment specific settings can be provided through a set of environment variables and embedded into the pipeline package at compile time.\n",
    "\n",
    "### Configure environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `GCP_REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_REGION = 'us-central1'\n",
    "ENDPOINT = '4693e549c20b349e-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://hostedkfp-default-cxlppwk5fs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "DATA_ROOT_URI = 'gs://workshop-datasets/covertype/small'\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/{}/covertype-tfx-image'.format(PROJECT_ID)\n",
    "PIPELINE_NAME = 'tfx_covertype_continuous_training'\n",
    "MODEL_NAME = 'tf_covertype_classifier'\n",
    "RUNTIME_VERSION = '2.1'\n",
    "PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=mlops-dev-101\n",
      "env: KUBEFLOW_TFX_IMAGE=gcr.io/mlops-dev-101/covertype-tfx-image\n",
      "env: ARTIFACT_STORE_URI=gs://hostedkfp-default-cxlppwk5fs\n",
      "env: DATA_ROOT_URI=gs://workshop-datasets/covertype/small\n",
      "env: GCP_REGION=us-central1\n",
      "env: MODEL_NAME=tf_covertype_classifier\n",
      "env: PIPELINE_NAME=tfx_covertype_continuous_training\n",
      "env: RUNTIME_VERSION=2.1\n",
      "env: PYTHON_VERIONS=3.7\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env KUBEFLOW_TFX_IMAGE={CUSTOM_TFX_IMAGE}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERIONS={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can build and upload the pipeline to the AI Platform Pipelines instance in one step, using the `tfx pipeline create` command. The `tfx pipeline create` goes through the following steps:\n",
    "- (Optional) Builds the custom image to host your components, \n",
    "- Compiles the pipeline DSL into a pipeline package \n",
    "- Uploads the pipeline package to the instance.\n",
    "\n",
    "As you debug the pipeline DSL, you may prefer to first use the `tfx pipeline compile` command, which only executes the compilation step. After the DSL compiles successfully you can use `tfx pipeline create` to go through all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path pipeline/runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package to AI Platform Pipelines\n",
    "\n",
    "After the pipeline code compiles without any errors you can use the `tfx pipeline create` command to perform the full build and deploy the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Use skaffold to build the container image.\n",
      "/home/jupyter/.local/bin/skaffold\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "Traceback (most recent call last):\n",
      "  File \"pipeline/runner.py\", line 20, in <module>\n",
      "    from tfx.orchestration.kubeflow import kubeflow_dag_runner\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/kubeflow/kubeflow_dag_runner.py\", line 31, in <module>\n",
      "    from tfx.orchestration import pipeline as tfx_pipeline\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/pipeline.py\", line 30, in <module>\n",
      "    from tfx.components.base import base_component\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/components/__init__.py\", line 20, in <module>\n",
      "    from tfx.components.bulk_inferrer.component import BulkInferrer\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/components/bulk_inferrer/component.py\", line 24, in <module>\n",
      "    from tfx.components.base import base_component\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/components/base/base_component.py\", line 29, in <module>\n",
      "    from tfx.components.base import base_node\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/components/base/base_node.py\", line 28, in <module>\n",
      "    from tfx.components.base import base_executor\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/components/base/base_executor.py\", line 29, in <module>\n",
      "    import apache_beam as beam\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/__init__.py\", line 97, in <module>\n",
      "    from apache_beam import coders\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/coders/__init__.py\", line 19, in <module>\n",
      "    from apache_beam.coders.coders import *\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/coders/coders.py\", line 30, in <module>\n",
      "    from future.moves import pickle\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/future/moves/__init__.py\", line 8, in <module>\n",
      "    import_top_level_modules()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/future/standard_library/__init__.py\", line 810, in import_top_level_modules\n",
      "    with exclude_local_folder_imports(*TOP_LEVEL_MODULES):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/future/standard_library/__init__.py\", line 781, in __enter__\n",
      "    module = __import__(m, level=0)\n",
      "  File \"/home/jupyter/mlops-labs/workshops/tfx-caip-tf21/lab-02-tfx-pipeline/pipeline/test.py\", line 18, in <module>\n",
      "    import tensorflow_model_analysis as tfma\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_model_analysis/__init__.py\", line 23, in <module>\n",
      "    from tensorflow_model_analysis.api import tfma_unit as test\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_model_analysis/api/tfma_unit.py\", line 67, in <module>\n",
      "    from apache_beam.testing import util as beam_util\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/testing/util.py\", line 29, in <module>\n",
      "    from apache_beam.transforms import window\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/transforms/__init__.py\", line 23, in <module>\n",
      "    from apache_beam.transforms import combiners\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/transforms/combiners.py\", line 41, in <module>\n",
      "    from apache_beam.transforms import core\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/transforms/core.py\", line 38, in <module>\n",
      "    from apache_beam.coders import typecoders\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/coders/typecoders.py\", line 183, in <module>\n",
      "    registry = CoderRegistry()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/coders/typecoders.py\", line 84, in __init__\n",
      "    self.register_standard_coders(fallback_coder)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/coders/typecoders.py\", line 88, in register_standard_coders\n",
      "    self._register_coder_internal(int, coders.VarIntCoder)\n",
      "AttributeError: module 'apache_beam.coders.coders' has no attribute 'VarIntCoder'\n",
      "Error while running \"/opt/conda/bin/python pipeline/runner.py\" \n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline_path=pipeline/runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Deleting pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"tfx_covertype_continuous_training\" does not exist.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and monitor a pipeline run\n",
    "After the pipeline has been deployed, you can trigger and monitor pipeline runs using TFX CLI or KFP UI.\n",
    "\n",
    "To submit the pipeline run using TFX CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run create --pipeline_name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all active runs of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run list --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID='[YOUR RUN ID]'\n",
    "\n",
    "!tfx run status --pipeline_name {PIPELINE_NAME} --run_id {RUN_ID} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
