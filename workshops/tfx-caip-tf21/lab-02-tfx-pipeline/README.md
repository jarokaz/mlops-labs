# Orchestrating model training and deployment with TFX and Cloud AI Platform

In this lab you will develop, deploy and run a TFX pipeline that uses Kubeflow Pipelines for orchestration and Cloud Dataflow and Cloud AI Platform for data processing, training, and deployment:


## Lab instructions

### Understanding the pipeline's DSL.

The pipeline implements a typical TFX workflow as depicted on the below diagram:

![Lab 14 diagram](/images/lab-14-diagram.png).

1. Training data in the CSV format is ingested from the GCS location using *CsvExampleGen*. The URI to the data root is passed as a runtime parameter. The `CsvExampleGen` component splits the source data into training and evaluation splits and converts the data into the TFRecords format.
2. The *StatisticsGen* component generates statistics for both splits.
3. The *ImporterNode* component is used to bring the schema file. The locations of the schema file is passed a runtime parameter. *This step is not depicted on the diagram*.
4. The *SchemaGen* component is used to autogenerate a schema . This is done for tracking. The schema imported byt the *ImporterNode* will be used by the downstream components like *ExampleValidator*. But we also want to capture the autogenerated schema for future analysis.
5. The *ExampleValidator* component is used to validate generated examples against the imported schema
6. The *Transform* component is used to preprocess the data and generate the preprocessing graph. The location of the Python script with the preprocessing code is passed as a runtime parameter
7. The *Trainer* component is used to start an AI Platform Training job. The AI Platform Training job is configured to use a custom training container. The AI Platform Training configuration, which includes the URI of the custom training image, is a compile time parameter. 
8. The *Evaluator* component is used to evaluate the trained model against the eval split
9. The *ModelValidate* component compares the model against a baseline. If this is the first run of the pipeline the model will be blessed regardless of an outcome of the evaluation. In subsequent runs, the baseline is a performance evaluation of the model from the previous run.
10. If the model trained by the pipeline was blessed, the model is deployed to AI Platform Prediction using the *Pusher* component.


The pipeline uses a custom docker image, which is a derivative of the [tensorflow/tfx:0.21.0 image](https://hub.docker.com/r/tensorflow/tfx), as a runtime execution environment for the pipeline's components. The same image is also used as a a training image used by **AI Platform Training**

The custom image modifies the base image by adding the  the `transform_train.py` file that contains data transformation and training code used by the pipeline's `Transform` and `Train` components.

### Configuring the environment settings

You will use a JupyterLab terminal terminal as the primary interface during the lab. Before proceeding with the lab exercises configure a set of environment variables that reflect your lab environment. If you used the default settings during the environment setup you don't need to modify the below commands. If you provided custom values for PREFIX, REGION, ZONE, or NAMESPACE update the commands accordingly:
```
export PROJECT_ID=$(gcloud config get-value core/project)
export PREFIX=$PROJECT_ID
export NAMESPACE=kubeflow
export GCP_REGION=us-central1
export ZONE=us-central1-a
export ARTIFACT_STORE_URI=gs://$PREFIX-artifact-store
export GCS_STAGING_PATH=${ARTIFACT_STORE_URI}/staging
export GKE_CLUSTER_NAME=$PREFIX-cluster
export DATA_ROOT_URI=gs://workshop-datasets/covertype/full

gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $ZONE
export INVERSE_PROXY_HOSTNAME=$(kubectl describe configmap inverse-proxy-config -n $NAMESPACE | grep "googleusercontent.com")
```

Follow the instructor who will walk you through the lab. The high level summary of the lab flow is as follows:




### Building and deploying the pipeline
#### Creating the custom docker image
The first step is to build the custom docker image and push it to your project's **Container Registry**. You will use **Cloud Build** to build the image.

1. Create the Dockerfile describing the custom image
```
cat > Dockerfile << EOF
FROM tensorflow/tfx:0.21.0
RUN mkdir modules
COPY  transform_train.py modules/
EOF
```

2. Submit the **Cloud Build** job
```
IMAGE_NAME=tfx-image
TAG=latest
export TFX_IMAGE="gcr.io/${PROJECT_ID}/${IMAGE_NAME}:${TAG}"

gcloud builds submit --timeout 15m --tag ${TFX_IMAGE} .
```

#### Building and uploading the pipeline to the KFP environment
You can build and upload the pipeline in one step, using the `tfx pipeline create` command. The `tfx pipeline create` can optionally build an image to host your components, compiles the pipeline DSL into the pipeline package and uploads the pipeline package to the KFP environment.

However; as you are debugging the pipeline DSL you may prefer to use the `tfx pipeline compile` which only executes the compilation step.

```


tfx pipeline create --engine kubeflow --pipeline_path pipeline_dsl.py --endpoint $INVERSE_PROXY_HOSTNAME
```


The `tfx pipeline create` command compiles the pipeline's DSL into the KFP package file - `tfx_covertype_classifier_training.tar.gz` and uploads the package to the KFP environment. The package file contains the description of the pipeline in the YAML format. If you want to examine the file, extract from the tarball file and use the JupyterLab editor.

```
tar xvf tfx_covertype_classifier_training.tar.gz
```

The name of the extracted file is `pipeline.yaml`.


### Submitting and monitoring pipeline runs

After the pipeline has been deployed, you can trigger and monitor pipeline runs using **TFX CLI** or **KFP UI**.

To submit the pipeline run using **TFX CLI**:
```
tfx run create --pipeline_name $PIPELINE_NAME --endpoint $INVERSE_PROXY_HOSTNAME
```

To list all the active runs of the pipeline:
```
tfx run list --pipeline_name $PIPELINE_NAME --endpoint $INVERSE_PROXY_HOSTNAME
```

To retrieve the status of a given run:
```
tfx run status --pipeline_name $PIPELINE_NAME --run_id [YOUR_RUN_ID] --endpoint $INVERSE_PROXY_HOSTNAME
```
 To terminate a run:
 ```
 tfx run terminate --run_id [YOUR_RUN_ID] --endpoint $INVERSE_PROXY_HOSTNAME
 ```


