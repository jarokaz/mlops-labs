{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Set `PATH` to include the directory containing TFX CLI.\n",
    "PATH=%env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for TFX pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a Cloud Build CI/CD workflow that automatically builds and deploys a TFX pipeline. You will also integrate your workflow with GitHub by setting up a trigger that starts the workflow when a new tag is applied to the GitHub repo hosting the pipeline's code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Cloud Build workflow\n",
    "Review the `cloudbuild.yaml` file to understand how the CI/CD workflow is implemented and how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The **Cloud Build** CI/CD workflow automates the steps you walked through manually during `lab-02`:\n",
    "1. Builds the custom TFX image to be used as a runtime execution environment for TFX components and as the AI Platform Training training container.\n",
    "1. Compiles the pipeline and uploads the pipeline to the KFP environment\n",
    "1. Pushes the custom TFX image to your project's **Container Registry**\n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **TFX CLI**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `GCP_REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_REGION = 'us-central1'\n",
    "ARTIFACT_STORE_URI = 'gs://hostedkfp-default-dcsh2ljqmk'\n",
    "ENDPOINT = '756f6ab12e557cde-dot-us-central2.pipelines.googleusercontent.com'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the TFX CLI builder\n",
    "### Review the Dockerfile describing the TFX CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-1\n",
      "COPY requirements.txt .\n",
      "RUN python -m pip install -U -r requirements.txt\n",
      "\n",
      "ENTRYPOINT [\"tfx\"]\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas<1.0.0\n",
      "tfx==0.21.2\n",
      "kfp==0.2.5\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='tfx-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the `gcloud builds submit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='tfx_covertype_training_deployment'\n",
    "TAG_NAME='test'\n",
    "TFX_IMAGE_NAME='lab-03-tfx-image'\n",
    "DATA_ROOT_URI='gs://workshop-datasets/covertype/small'\n",
    "MODEL_NAME='tfx_covertype_classifier'\n",
    "PIPELINE_FOLDER='lab-02-tfx-pipeline'\n",
    "PIPELINE_DSL='pipeline/runner.py'\n",
    "RUNTIME_VERSION='2.1'\n",
    "PYTHON_VERSION='3.7'\n",
    "\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_GCP_REGION={},\\\n",
    "_ARTIFACT_STORE_URI={},\\\n",
    "_TFX_IMAGE_NAME={},\\\n",
    "_DATA_ROOT_URI={},\\\n",
    "_MODEL_NAME={},\\\n",
    "TAG_NAME={},\\\n",
    "_PIPELINE_FOLDER={},\\\n",
    "_PIPELINE_DSL={},\\\n",
    "_PIPELINE_NAME={},\\\n",
    "_RUNTIME_VERSION={},\\\n",
    "_PYTHON_VERSION={}\n",
    "\"\"\".format(ENDPOINT, \n",
    "           GCP_REGION, \n",
    "           ARTIFACT_STORE_URI, \n",
    "           TFX_IMAGE_NAME,\n",
    "           DATA_ROOT_URI,\n",
    "           MODEL_NAME,\n",
    "           TAG_NAME, \n",
    "           PIPELINE_FOLDER,\n",
    "           PIPELINE_DSL,\n",
    "           PIPELINE_NAME,\n",
    "           RUNTIME_VERSION,\n",
    "           PYTHON_VERSION\n",
    "           ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 41 file(s) totalling 213.6 KiB before compression.\n",
      "Uploading tarball of [..] to [gs://mlops-workshop_cloudbuild/source/1585164063.76-623be71616554c8c8f44b4692acc9ba5.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/d8d6e6b3-2db7-4cfe-ac1a-42932d975619].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/d8d6e6b3-2db7-4cfe-ac1a-42932d975619?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d8d6e6b3-2db7-4cfe-ac1a-42932d975619\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1585164063.76-623be71616554c8c8f44b4692acc9ba5.tgz#1585164064157796\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1585164063.76-623be71616554c8c8f44b4692acc9ba5.tgz#1585164064157796...\n",
      "/ [1 files][ 50.1 KiB/ 50.1 KiB]                                                \n",
      "Operation completed over 1 objects/50.1 KiB.                                     \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  100.4kB\n",
      "Step #0: Step 1/5 : FROM tensorflow/tfx:0.21.2\n",
      "Step #0: 0.21.2: Pulling from tensorflow/tfx\n",
      "Step #0: fe703b657a32: Pulling fs layer\n",
      "Step #0: f9df1fafd224: Pulling fs layer\n",
      "Step #0: a645a4b887f9: Pulling fs layer\n",
      "Step #0: 57db7fe0b522: Pulling fs layer\n",
      "Step #0: b11ab658a92d: Pulling fs layer\n",
      "Step #0: 16211b60a645: Pulling fs layer\n",
      "Step #0: 3bb944b07b6e: Pulling fs layer\n",
      "Step #0: 791f84056415: Pulling fs layer\n",
      "Step #0: 3d7f8829d670: Pulling fs layer\n",
      "Step #0: 63c6308d7125: Pulling fs layer\n",
      "Step #0: e7076ce00cca: Pulling fs layer\n",
      "Step #0: 084f6df051b9: Pulling fs layer\n",
      "Step #0: 57db7fe0b522: Waiting\n",
      "Step #0: b11ab658a92d: Waiting\n",
      "Step #0: 16211b60a645: Waiting\n",
      "Step #0: 3bb944b07b6e: Waiting\n",
      "Step #0: 791f84056415: Waiting\n",
      "Step #0: 3d7f8829d670: Waiting\n",
      "Step #0: 63c6308d7125: Waiting\n",
      "Step #0: e7076ce00cca: Waiting\n",
      "Step #0: 084f6df051b9: Waiting\n",
      "Step #0: a645a4b887f9: Verifying Checksum\n",
      "Step #0: a645a4b887f9: Download complete\n",
      "Step #0: f9df1fafd224: Verifying Checksum\n",
      "Step #0: f9df1fafd224: Download complete\n",
      "Step #0: fe703b657a32: Verifying Checksum\n",
      "Step #0: fe703b657a32: Download complete\n",
      "Step #0: 57db7fe0b522: Verifying Checksum\n",
      "Step #0: 57db7fe0b522: Download complete\n",
      "Step #0: 16211b60a645: Verifying Checksum\n",
      "Step #0: 16211b60a645: Download complete\n",
      "Step #0: 3bb944b07b6e: Verifying Checksum\n",
      "Step #0: 3bb944b07b6e: Download complete\n",
      "Step #0: b11ab658a92d: Verifying Checksum\n",
      "Step #0: b11ab658a92d: Download complete\n",
      "Step #0: 3d7f8829d670: Verifying Checksum\n",
      "Step #0: 3d7f8829d670: Download complete\n",
      "Step #0: e7076ce00cca: Verifying Checksum\n",
      "Step #0: e7076ce00cca: Download complete\n",
      "Step #0: 63c6308d7125: Verifying Checksum\n",
      "Step #0: 63c6308d7125: Download complete\n",
      "Step #0: 084f6df051b9: Verifying Checksum\n",
      "Step #0: 084f6df051b9: Download complete\n",
      "Step #0: 791f84056415: Verifying Checksum\n",
      "Step #0: 791f84056415: Download complete\n",
      "Step #0: fe703b657a32: Pull complete\n",
      "Step #0: f9df1fafd224: Pull complete\n",
      "Step #0: a645a4b887f9: Pull complete\n",
      "Step #0: 57db7fe0b522: Pull complete\n",
      "Step #0: b11ab658a92d: Pull complete\n",
      "Step #0: 16211b60a645: Pull complete\n",
      "Step #0: 3bb944b07b6e: Pull complete\n",
      "Step #0: 791f84056415: Pull complete\n",
      "Step #0: 3d7f8829d670: Pull complete\n",
      "Step #0: 63c6308d7125: Pull complete\n",
      "Step #0: e7076ce00cca: Pull complete\n",
      "Step #0: 084f6df051b9: Pull complete\n",
      "Step #0: Digest: sha256:57bb5d63a6a73912fcf821b91e2e5bc0ab7f730201038b407a644d182769d075\n",
      "Step #0: Status: Downloaded newer image for tensorflow/tfx:0.21.2\n",
      "Step #0:  ---> 131efa6e43e7\n",
      "Step #0: Step 2/5 : WORKDIR /pipeline\n",
      "Step #0:  ---> Running in 7979ff53a475\n",
      "Step #0: Removing intermediate container 7979ff53a475\n",
      "Step #0:  ---> 837ba0254a18\n",
      "Step #0: Step 3/5 : COPY pipeline/* ./\n",
      "Step #0:  ---> b246f1263da1\n",
      "Step #0: Step 4/5 : COPY schema/schema.pbtxt ./schema/\n",
      "Step #0:  ---> 8e06cbe3f390\n",
      "Step #0: Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "Step #0:  ---> Running in 347e60433fe2\n",
      "Step #0: Removing intermediate container 347e60433fe2\n",
      "Step #0:  ---> 573dad9e45af\n",
      "Step #0: Successfully built 573dad9e45af\n",
      "Step #0: Successfully tagged gcr.io/mlops-workshop/lab-03-tfx-image:test\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Pulling image: gcr.io/mlops-workshop/tfx-cli\n",
      "Step #1: Using default tag: latest\n",
      "Step #1: latest: Pulling from mlops-workshop/tfx-cli\n",
      "Step #1: 423ae2b273f4: Already exists\n",
      "Step #1: de83a2304fa1: Already exists\n",
      "Step #1: f9a83bce3af0: Already exists\n",
      "Step #1: b6b53be908de: Already exists\n",
      "Step #1: b2712ee52c8a: Pulling fs layer\n",
      "Step #1: ea5f203ec613: Pulling fs layer\n",
      "Step #1: 03c59c37590c: Pulling fs layer\n",
      "Step #1: 7f1f0de4c26d: Pulling fs layer\n",
      "Step #1: acbb498b3663: Pulling fs layer\n",
      "Step #1: e902a9253d07: Pulling fs layer\n",
      "Step #1: f840da7abf6f: Pulling fs layer\n",
      "Step #1: 7df56b5a1b90: Pulling fs layer\n",
      "Step #1: a2724a388c5b: Pulling fs layer\n",
      "Step #1: 444a423384b3: Pulling fs layer\n",
      "Step #1: c8782b778b9f: Pulling fs layer\n",
      "Step #1: 8e8ac5d0d6f3: Pulling fs layer\n",
      "Step #1: 455723d7f082: Pulling fs layer\n",
      "Step #1: f1b3fcc9bb5e: Pulling fs layer\n",
      "Step #1: 11000bbb250a: Pulling fs layer\n",
      "Step #1: 7f1f0de4c26d: Waiting\n",
      "Step #1: acbb498b3663: Waiting\n",
      "Step #1: e902a9253d07: Waiting\n",
      "Step #1: f840da7abf6f: Waiting\n",
      "Step #1: 7df56b5a1b90: Waiting\n",
      "Step #1: a2724a388c5b: Waiting\n",
      "Step #1: 444a423384b3: Waiting\n",
      "Step #1: c8782b778b9f: Waiting\n",
      "Step #1: 8e8ac5d0d6f3: Waiting\n",
      "Step #1: 455723d7f082: Waiting\n",
      "Step #1: f1b3fcc9bb5e: Waiting\n",
      "Step #1: 11000bbb250a: Waiting\n",
      "Step #1: 03c59c37590c: Verifying Checksum\n",
      "Step #1: 03c59c37590c: Download complete\n",
      "Step #1: ea5f203ec613: Verifying Checksum\n",
      "Step #1: ea5f203ec613: Download complete\n",
      "Step #1: 7f1f0de4c26d: Verifying Checksum\n",
      "Step #1: 7f1f0de4c26d: Download complete\n",
      "Step #1: acbb498b3663: Verifying Checksum\n",
      "Step #1: acbb498b3663: Download complete\n",
      "Step #1: e902a9253d07: Verifying Checksum\n",
      "Step #1: e902a9253d07: Download complete\n",
      "Step #1: f840da7abf6f: Verifying Checksum\n",
      "Step #1: f840da7abf6f: Download complete\n",
      "Step #1: 7df56b5a1b90: Verifying Checksum\n",
      "Step #1: 7df56b5a1b90: Download complete\n",
      "Step #1: 444a423384b3: Verifying Checksum\n",
      "Step #1: 444a423384b3: Download complete\n",
      "Step #1: a2724a388c5b: Verifying Checksum\n",
      "Step #1: a2724a388c5b: Download complete\n",
      "Step #1: 8e8ac5d0d6f3: Verifying Checksum\n",
      "Step #1: 8e8ac5d0d6f3: Download complete\n",
      "Step #1: 455723d7f082: Verifying Checksum\n",
      "Step #1: 455723d7f082: Download complete\n",
      "Step #1: f1b3fcc9bb5e: Verifying Checksum\n",
      "Step #1: f1b3fcc9bb5e: Download complete\n",
      "Step #1: b2712ee52c8a: Verifying Checksum\n",
      "Step #1: b2712ee52c8a: Download complete\n",
      "Step #1: 11000bbb250a: Verifying Checksum\n",
      "Step #1: 11000bbb250a: Download complete\n",
      "Step #1: c8782b778b9f: Download complete\n",
      "Step #1: b2712ee52c8a: Pull complete\n",
      "Step #1: ea5f203ec613: Pull complete\n",
      "Step #1: 03c59c37590c: Pull complete\n",
      "Step #1: 7f1f0de4c26d: Pull complete\n",
      "Step #1: acbb498b3663: Pull complete\n",
      "Step #1: e902a9253d07: Pull complete\n",
      "Step #1: f840da7abf6f: Pull complete\n",
      "Step #1: 7df56b5a1b90: Pull complete\n",
      "Step #1: a2724a388c5b: Pull complete\n",
      "Step #1: 444a423384b3: Pull complete\n",
      "Step #1: c8782b778b9f: Pull complete\n",
      "Step #1: 8e8ac5d0d6f3: Pull complete\n",
      "Step #1: 455723d7f082: Pull complete\n",
      "Step #1: f1b3fcc9bb5e: Pull complete\n",
      "Step #1: 11000bbb250a: Pull complete\n",
      "Step #1: Digest: sha256:a169769efc7a63b02d7121304fa62b9b9e41bb152f1243df099727ad139c4d5e\n",
      "Step #1: Status: Downloaded newer image for gcr.io/mlops-workshop/tfx-cli:latest\n",
      "Step #1: gcr.io/mlops-workshop/tfx-cli:latest\n",
      "Step #1: CLI\n",
      "Step #1: Creating pipeline\n",
      "Step #1: New container image is built. Target image is available in the build spec file.\n",
      "Step #1: /opt/conda/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "Step #1:   warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "Step #1: Pipeline compiled successfully.\n",
      "Step #1: Pipeline package path: /workspace/lab-02-tfx-pipeline/tfx_covertype_training_deployment-test.tar.gz\n",
      "Step #1: {'created_at': datetime.datetime(2020, 3, 25, 19, 26, 33, tzinfo=tzlocal()),\n",
      "Step #1:  'default_version': {'code_source_url': None,\n",
      "Step #1:                      'created_at': datetime.datetime(2020, 3, 25, 19, 26, 33, tzinfo=tzlocal()),\n",
      "Step #1:                      'id': 'a5aff5d6-1f47-47d8-8d40-47f062e3e47e',\n",
      "Step #1:                      'name': 'tfx_covertype_training_deployment-test',\n",
      "Step #1:                      'package_url': None,\n",
      "Step #1:                      'parameters': [{'name': 'pipeline-root',\n",
      "Step #1:                                      'value': 'gs://hostedkfp-default-dcsh2ljqmk/tfx_covertype_training_deployment-test/{{workflow.uid}}'},\n",
      "Step #1:                                     {'name': 'data-root-uri',\n",
      "Step #1:                                      'value': 'gs://workshop-datasets/covertype/small'},\n",
      "Step #1:                                     {'name': 'eval-steps', 'value': '500'},\n",
      "Step #1:                                     {'name': 'train-steps', 'value': '500'}],\n",
      "Step #1:                      'resource_references': [{'key': {'id': 'a5aff5d6-1f47-47d8-8d40-47f062e3e47e',\n",
      "Step #1:                                                       'type': 'PIPELINE'},\n",
      "Step #1:                                               'name': None,\n",
      "Step #1:                                               'relationship': 'OWNER'}]},\n",
      "Step #1:  'description': None,\n",
      "Step #1:  'error': None,\n",
      "Step #1:  'id': 'a5aff5d6-1f47-47d8-8d40-47f062e3e47e',\n",
      "Step #1:  'name': 'tfx_covertype_training_deployment-test',\n",
      "Step #1:  'parameters': [{'name': 'pipeline-root',\n",
      "Step #1:                  'value': 'gs://hostedkfp-default-dcsh2ljqmk/tfx_covertype_training_deployment-test/{{workflow.uid}}'},\n",
      "Step #1:                 {'name': 'data-root-uri',\n",
      "Step #1:                  'value': 'gs://workshop-datasets/covertype/small'},\n",
      "Step #1:                 {'name': 'eval-steps', 'value': '500'},\n",
      "Step #1:                 {'name': 'train-steps', 'value': '500'}],\n",
      "Step #1:  'url': None}\n",
      "Step #1: Pipeline \"tfx_covertype_training_deployment-test\" created successfully.\n",
      "Finished Step #1\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/lab-03-tfx-image:test\n",
      "The push refers to repository [gcr.io/mlops-workshop/lab-03-tfx-image]\n",
      "da8bbd46d437: Preparing\n",
      "c68daf6ecf3a: Preparing\n",
      "32978b859e0c: Preparing\n",
      "9c8b3c84966c: Preparing\n",
      "69e865282a57: Preparing\n",
      "39babeaf2809: Preparing\n",
      "1e39c57dcbc0: Preparing\n",
      "1d9b5c27dacf: Preparing\n",
      "34b845e6f8b5: Preparing\n",
      "d6b9841f1545: Preparing\n",
      "89cb746d479d: Preparing\n",
      "4ae3adcb66cb: Preparing\n",
      "aa6685385151: Preparing\n",
      "0040d8f00d7e: Preparing\n",
      "9e6f810a2aab: Preparing\n",
      "39babeaf2809: Waiting\n",
      "1e39c57dcbc0: Waiting\n",
      "1d9b5c27dacf: Waiting\n",
      "34b845e6f8b5: Waiting\n",
      "d6b9841f1545: Waiting\n",
      "89cb746d479d: Waiting\n",
      "4ae3adcb66cb: Waiting\n",
      "aa6685385151: Waiting\n",
      "0040d8f00d7e: Waiting\n",
      "9e6f810a2aab: Waiting\n",
      "69e865282a57: Layer already exists\n",
      "9c8b3c84966c: Layer already exists\n",
      "39babeaf2809: Layer already exists\n",
      "1e39c57dcbc0: Layer already exists\n",
      "34b845e6f8b5: Layer already exists\n",
      "1d9b5c27dacf: Layer already exists\n",
      "d6b9841f1545: Layer already exists\n",
      "89cb746d479d: Layer already exists\n",
      "4ae3adcb66cb: Layer already exists\n",
      "aa6685385151: Layer already exists\n",
      "0040d8f00d7e: Layer already exists\n",
      "9e6f810a2aab: Layer already exists\n",
      "c68daf6ecf3a: Pushed\n",
      "32978b859e0c: Pushed\n",
      "da8bbd46d437: Pushed\n",
      "test: digest: sha256:d39502fbecf2de7a441ba730c214c8e3ec2a4c2dbdc566232c6487ca2577755f size: 3466\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                       STATUS\n",
      "d8d6e6b3-2db7-4cfe-ac1a-42932d975619  2020-03-25T19:21:04+00:00  5M37S     gs://mlops-workshop_cloudbuild/source/1585164063.76-623be71616554c8c8f44b4692acc9ba5.tgz  gcr.io/mlops-workshop/lab-03-tfx-image:test  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit .. --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location|/ workshops/kfp-caip-sklearn/lab-03-tfx-cicd/cloudbuild.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_BASE_IMAGE_NAME|base_image|\n",
    "|_COMPONENT_URL_SEARCH_PREFIX|https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/|\n",
    "|_ENDPOINT|[Your inverting proxy host]|\n",
    "|_PIPELINE_DSL|covertype_training_pipeline.py|\n",
    "|_PIPELINE_FOLDER|workshops/kfp-caip-sklearn/lab-03-kfp-cicd/pipeline|\n",
    "|_PIPELINE_NAME|covertype_training_deployment|\n",
    "|_PIPELINE_PACKAGE|covertype_training_pipeline.yaml|\n",
    "|_PYTHON_VERSION|3.7|\n",
    "|_RUNTIME_VERSION|1.15|\n",
    "|_TRAINER_IMAGE_NAME|trainer_image|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
