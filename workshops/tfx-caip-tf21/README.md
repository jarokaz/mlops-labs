# Continuous training with TFX and Cloud AI Platform

This series of hands on labs guides you through the process of implementing a TensorFlow Extended (TFX) continuous training pipeline that automates training and deployment of a TensorFlow 2.1 model.

The below diagram represents the workflow orchestrated by the pipeline.

![TFX_CAIP](/images/tfx-caip.png).

1. Training data in the CSV format is ingested from the GCS location using *CsvExampleGen*. The URI to the data root is passed as a runtime parameter. The `CsvExampleGen` component splits the source data into training and evaluation splits and converts the data into the TFRecords format.
2. The *StatisticsGen* component generates statistics for both splits.
3. The *ImporterNode* component is used to bring the schema file. The locations of the schema file is passed as a runtime parameter. 
4. The *SchemaGen* component autogenerates a new schema . This is done for tracking. The schema imported byt the *ImporterNode* will be used by the downstream components like *ExampleValidator*. But we also want to capture the autogenerated schema for future analysis.
5. The *ExampleValidator* component validates the generated examples against the imported schema
6. The *Transform* component preprocess the data to the format required by the *Trainer* compoment. It also saves the preprocessing TensorFlow graph. 
7. The *Trainer* starts an AI Platform Training job. The AI Platform Training job is configured to use a custom training container. The AI Platform Training configuration, which includes the URI of the custom training image, is a compile time setting. 
8. The *Evaluator* component evaluates the trained model against the eval split
9. The *ModelValidate* component compares the model against a baseline. If this is the first run of the pipeline the model will be blessed regardless of an outcome of the evaluation. In subsequent runs, the baseline is the performance the model from the previous runs.
10. If the model trained by the pipeline is blessed, the model is deployed to AI Platform Prediction using the *Pusher* component. The configuration of AI Platform Prediction is a compile time setting.

The ML model utilized in the labs  is a multi-class classifier that predicts the type of  forest cover from cartographic data. The model is trained on the [Covertype Data Set](/datasets/covertype/README.md) dataset.

## Preparing the lab environment
You will use the lab environment configured as on the below diagram:

![Lab env](/images/lab-env.png)

The core services in the environment are:
- ML experimentation and development - AI Platform Notebooks 
- Scalable, serverless model training - AI Platform Training  
- Scalable, serverless model serving - AI Platform Prediction 
- Machine learning pipelines - AI Platform Pipelines
- Distributed data processing - Cloud Dataflow  
- Analytics data warehouse - BigQuery 
- Artifact store - Google Cloud Storage 
- CI/CD tooling - Cloud Build
    
In this environment, all services are provisioned in the same [Google Cloud Project](https://cloud.google.com/storage/docs/projects). 

### Enabling Cloud Services

To enable Cloud Services utilized in the lab environment:
1. Launch [Cloud Shell](https://cloud.google.com/shell/docs/launching-cloud-shell)
2. Set your project ID
```
PROJECT_ID=[YOUR PROJECT ID]

gcloud config set project $PROJECT_ID
```
3. Use `gcloud` to enable the services
```
gcloud services enable automl.googleapis.com
gcloud services enable \
cloudbuild.googleapis.com \
container.googleapis.com \
cloudresourcemanager.googleapis.com \
iam.googleapis.com \
containerregistry.googleapis.com \
containeranalysis.googleapis.com \
ml.googleapis.com \
dataflow.googleapis.com 
```
### Creating an instance of AI Platform Pipelines
The core component of the lab environment is **AI Platform Pipelines**. To create an instance of **AI Platform Pipelines** follow the [Setting up AI Platform Pipelines](https://cloud.google.com/ai-platform/pipelines/docs/setting-up) how-to guide.

### Creating an artifact store
In the lab environment, Google Cloud Storage is used as an artifact store. By default, the name of the bucket is set to `[PROJECT_ID]-artifact-store`. To create the bucket, execute the following command from **Cloud Shell**:

```
BUCKET_NAME=gs://${PROJECT_ID}-artifact-store
gsutil mb $BUCKET_NAME
```


### Creating an instance of AI Platform Notebooks

An instance of **AI Platform Notebooks** is used as a primary experimentation/development workbench. The instance is configured using a custom container image that includes all Python packages required for the hands-on labs. 


1. In [Cloud Shell](https://cloud.google.com/shell/docs/launching-cloud-shell), create a folder in your `home` directory
```
cd
mkdir tmp-workspace
cd tmp-workspace
```
2. Create the requirements file with the Python packages to install in the custom image
```
cat > requirements.txt << EOF
pandas<1
tfx==0.21
kfp==0.2.5
tensorboard~=2.1.0
gcsfs
EOF
```
3. Create the Dockerfile defining you custom container image
```
cat > Dockerfile << EOF
FROM gcr.io/deeplearning-platform-release/base-cpu
SHELL ["/bin/bash", "-c"]
RUN apt-get update -y && apt-get -y install kubectl
RUN curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 \
&& chmod +x skaffold \
&& mv skaffold /usr/local/bin
COPY requirements.txt .
RUN conda create -n tfx python=3.7 && source activate tfx \
&& python -m pip install -U -r requirements.txt \
&& python -m ipykernel install --name tfx 
EOF
```
4. Build the image and push it to your project's **Container Registry**
```
IMAGE_NAME=mlops-dev
TAG=latest
IMAGE_URI="gcr.io/${PROJECT_ID}/${IMAGE_NAME}:${TAG}"

gcloud builds submit --timeout 15m --tag ${IMAGE_URI} .
```
5. Create an instance of **AI Platform Notebooks**. Note that you can also create an instance using [GCP Console](https://cloud.google.com/ai-platform/notebooks/docs/custom-container)
```
ZONE=[YOUR_ZONE]
INSTANCE_NAME=[YOUR_INSTANCE_NAME]

IMAGE_FAMILY="common-container"
IMAGE_PROJECT="deeplearning-platform-release"
INSTANCE_TYPE="n1-standard-4"
METADATA="proxy-mode=service_account,container=$IMAGE_URI"

gcloud compute instances create $INSTANCE_NAME \
    --zone=$ZONE \
    --image-family=$IMAGE_FAMILY \
    --machine-type=$INSTANCE_TYPE \
    --image-project=$IMAGE_PROJECT \
    --maintenance-policy=TERMINATE \
    --boot-disk-device-name=${INSTANCE_NAME}-disk \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-ssd \
    --scopes=cloud-platform,userinfo-email \
    --metadata=$METADATA
```

6. After the instance is created, you can connect to [JupyterLab](https://jupyter.org/) IDE by clicking the *OPEN JUPYTERLAB* link in the [AI Platform Notebooks Console](https://console.cloud.google.com/ai-platform/notebooks/instances).

7. In the **JupyterLab**, open a terminal and clone this repository in the `home` folder.
```
cd home
git clone https://github.com/jarokaz/mlops-labs.git
```


## Summary of lab exercises

### Lab-01 - TFX Components walk-through
In this lab you will step through the configuration and execution of core TFX Components using the TFX interactive context. The primary goal of the lab is to get a high level understanding of a function and usage of each of the components. 

### Lab-02 - Orchestrating model training and deployment with Kubeflow Pipelines and Cloud AI Platform
In this lab you will develop, deploy and run a TFX pipeline that uses  Cloud Dataflow and Cloud AI Platform as execution runtimes.

### Lab-03 - CI/CD for a KFP pipeline
In this lab you will author a **Cloud Build** CI/CD workflow that automatically builds and deploys a TFX pipeline. You will also integrate your workflow with **GitHub**.

