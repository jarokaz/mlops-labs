{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for a KFP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a **Cloud Build** CI/CD workflow that automatically builds and deploys a KFP pipeline. You will also integrate your workflow with **GitHub** by setting up a trigger that starts the  workflow when a new tag is applied to the **GitHub** repo hosting the pipeline's code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the **Cloud Build** workflow.\n",
    "\n",
    "Review the `cloudbuild.yaml` file to understand how the CI/CD workflow is implemented and how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The CI/CD workflow automates the steps you walked through manually during `lab-02-kfp-pipeline`:\n",
    "1. Builds the trainer image\n",
    "1. Builds the base image for custom components\n",
    "1. Compiles the pipeline\n",
    "1. Uploads the pipeline to the KFP environment\n",
    "1. Pushes the trainer and base images to your project's **Container Registry**\n",
    "\n",
    "Although the KFP backend supports pipeline versioning, this feature has not been yet enable through the **KFP** CLI. As a temporary workaround, in the **Cloud Build** configuration a value of the `TAG_NAME` variable is appended to the name of the pipeline. \n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **KFP CLI**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings\n",
    "\n",
    "Update  the `ENDPOINT` constat  with the settings reflecting your lab environment. \n",
    "\n",
    "Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = '756f6ab12e557cde-dot-us-central2.pipelines.googleusercontent.com'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the KFP CLI builder\n",
    "### Review the Dockerfile describing the KFP CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install kfp==0.2.5\n",
      "ENTRYPOINT [\"/bin/bash\"]\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the `gcloud builds submit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_TRAINER_IMAGE_NAME=trainer_image,\\\n",
    "_BASE_IMAGE_NAME=base_image,\\\n",
    "TAG_NAME=test,\\\n",
    "_PIPELINE_FOLDER=lab-02-kfp-pipeline,\\\n",
    "_PIPELINE_DSL=covertype_training_pipeline.py,\\\n",
    "_PIPELINE_PACKAGE=covertype_training_pipeline.yaml,\\\n",
    "_PIPELINE_NAME=covertype_training_deployment,\\\n",
    "_RUNTIME_VERSION=1.15,\\\n",
    "_PYTHON_VERSION=3.7,\\\n",
    "_COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
    "\"\"\".format(ENDPOINT).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 26 file(s) totalling 98.7 KiB before compression.\n",
      "Uploading tarball of [..] to [gs://mlops-workshop_cloudbuild/source/1585170143.86-e219a711d25c4d669e72054429bdfbdd.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/2c0071fb-65f8-4c51-ba77-3979861853b9].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/2c0071fb-65f8-4c51-ba77-3979861853b9?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2c0071fb-65f8-4c51-ba77-3979861853b9\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1585170143.86-e219a711d25c4d669e72054429bdfbdd.tgz#1585170144269390\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1585170143.86-e219a711d25c4d669e72054429bdfbdd.tgz#1585170144269390...\n",
      "/ [1 files][ 21.0 KiB/ 21.0 KiB]                                                \n",
      "Operation completed over 1 objects/21.0 KiB.                                     \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  6.144kB\n",
      "Step #0: Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0: 423ae2b273f4: Already exists\n",
      "Step #0: de83a2304fa1: Already exists\n",
      "Step #0: f9a83bce3af0: Already exists\n",
      "Step #0: b6b53be908de: Already exists\n",
      "Step #0: 5650063cfbfb: Pulling fs layer\n",
      "Step #0: 89142850430d: Pulling fs layer\n",
      "Step #0: 498b10157bcd: Pulling fs layer\n",
      "Step #0: a77a3b1caf74: Pulling fs layer\n",
      "Step #0: 0603289dda03: Pulling fs layer\n",
      "Step #0: c3ae245b40c1: Pulling fs layer\n",
      "Step #0: 67e85692af8b: Pulling fs layer\n",
      "Step #0: ea72ab3b7167: Pulling fs layer\n",
      "Step #0: b02850f0d90c: Pulling fs layer\n",
      "Step #0: 4295de6959ce: Pulling fs layer\n",
      "Step #0: d651a7c122d6: Pulling fs layer\n",
      "Step #0: 69e0b993e5f5: Pulling fs layer\n",
      "Step #0: a77a3b1caf74: Waiting\n",
      "Step #0: 0603289dda03: Waiting\n",
      "Step #0: c3ae245b40c1: Waiting\n",
      "Step #0: 67e85692af8b: Waiting\n",
      "Step #0: ea72ab3b7167: Waiting\n",
      "Step #0: b02850f0d90c: Waiting\n",
      "Step #0: 4295de6959ce: Waiting\n",
      "Step #0: 69e0b993e5f5: Waiting\n",
      "Step #0: d651a7c122d6: Waiting\n",
      "Step #0: 498b10157bcd: Verifying Checksum\n",
      "Step #0: 498b10157bcd: Download complete\n",
      "Step #0: 89142850430d: Verifying Checksum\n",
      "Step #0: 89142850430d: Download complete\n",
      "Step #0: a77a3b1caf74: Verifying Checksum\n",
      "Step #0: a77a3b1caf74: Download complete\n",
      "Step #0: c3ae245b40c1: Verifying Checksum\n",
      "Step #0: c3ae245b40c1: Download complete\n",
      "Step #0: 67e85692af8b: Verifying Checksum\n",
      "Step #0: 67e85692af8b: Download complete\n",
      "Step #0: ea72ab3b7167: Verifying Checksum\n",
      "Step #0: ea72ab3b7167: Download complete\n",
      "Step #0: b02850f0d90c: Verifying Checksum\n",
      "Step #0: b02850f0d90c: Download complete\n",
      "Step #0: 4295de6959ce: Verifying Checksum\n",
      "Step #0: 4295de6959ce: Download complete\n",
      "Step #0: d651a7c122d6: Verifying Checksum\n",
      "Step #0: d651a7c122d6: Download complete\n",
      "Step #0: 69e0b993e5f5: Verifying Checksum\n",
      "Step #0: 69e0b993e5f5: Download complete\n",
      "Step #0: 5650063cfbfb: Verifying Checksum\n",
      "Step #0: 5650063cfbfb: Download complete\n",
      "Step #0: 0603289dda03: Verifying Checksum\n",
      "Step #0: 0603289dda03: Download complete\n",
      "Step #0: 5650063cfbfb: Pull complete\n",
      "Step #0: 89142850430d: Pull complete\n",
      "Step #0: 498b10157bcd: Pull complete\n",
      "Step #0: a77a3b1caf74: Pull complete\n",
      "Step #0: 0603289dda03: Pull complete\n",
      "Step #0: c3ae245b40c1: Pull complete\n",
      "Step #0: 67e85692af8b: Pull complete\n",
      "Step #0: ea72ab3b7167: Pull complete\n",
      "Step #0: b02850f0d90c: Pull complete\n",
      "Step #0: 4295de6959ce: Pull complete\n",
      "Step #0: d651a7c122d6: Pull complete\n",
      "Step #0: 69e0b993e5f5: Pull complete\n",
      "Step #0: Digest: sha256:d052cdcfacd704995a3a54650f43aeba5228228a66af67833304b6e8a18060f1\n",
      "Step #0: Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0:  ---> b9a9911547cc\n",
      "Step #0: Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0:  ---> Running in 602bbffb52fe\n",
      "Step #0: Collecting fire\n",
      "Step #0:   Downloading fire-0.3.0.tar.gz (81 kB)\n",
      "Step #0: Collecting cloudml-hypertune\n",
      "Step #0:   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0: Collecting scikit-learn==0.20.4\n",
      "Step #0:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0: Collecting pandas==0.24.2\n",
      "Step #0:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.1)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.4.1)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2019.3)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #0: Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.3.0-py2.py3-none-any.whl size=111108 sha256=cfe4720304e031b51ae605e2263d8a5ed758c30a7e6937acd2116697a89933be\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/2c/91/7b/2954fc66c99d47e663eac5d22a6b5a9bf39a230fa80e6782e8\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=e34b6246e140fb61f9e361f3fe3b6d0a154c5744b7e37c8675a9ce52a0b99ffe\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0:   Building wheel for termcolor (setup.py): started\n",
      "Step #0:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=3b166b242236c5e471a4089ac63a8346b7dcb920b1bca39193cdb906562ae2d1\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #0: Successfully built fire cloudml-hypertune termcolor\n",
      "Step #0: \u001b[91mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\n",
      "Step #0: \u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "Step #0:   Attempting uninstall: scikit-learn\n",
      "Step #0:     Found existing installation: scikit-learn 0.22.1\n",
      "Step #0:     Uninstalling scikit-learn-0.22.1:\n",
      "Step #0:       Successfully uninstalled scikit-learn-0.22.1\n",
      "Step #0:   Attempting uninstall: pandas\n",
      "Step #0:     Found existing installation: pandas 1.0.1\n",
      "Step #0:     Uninstalling pandas-1.0.1:\n",
      "Step #0:       Successfully uninstalled pandas-1.0.1\n",
      "Step #0: Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Step #0: Removing intermediate container 602bbffb52fe\n",
      "Step #0:  ---> e50deac5d017\n",
      "Step #0: Step 3/5 : WORKDIR /app\n",
      "Step #0:  ---> Running in f7170bc0f515\n",
      "Step #0: Removing intermediate container f7170bc0f515\n",
      "Step #0:  ---> f12db05bce46\n",
      "Step #0: Step 4/5 : COPY train.py .\n",
      "Step #0:  ---> 1f19a64449f6\n",
      "Step #0: Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0:  ---> Running in ca8510fa78eb\n",
      "Step #0: Removing intermediate container ca8510fa78eb\n",
      "Step #0:  ---> 9d1dbfeb4cfe\n",
      "Step #0: Successfully built 9d1dbfeb4cfe\n",
      "Step #0: Successfully tagged gcr.io/mlops-workshop/trainer_image:test\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Sending build context to Docker daemon  2.048kB\n",
      "Step #1: Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #1:  ---> b9a9911547cc\n",
      "Step #1: Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      "Step #1:  ---> Running in 31ea7c1e0dbe\n",
      "Step #1: Collecting fire\n",
      "Step #1:   Downloading fire-0.3.0.tar.gz (81 kB)\n",
      "Step #1: Collecting scikit-learn==0.20.4\n",
      "Step #1:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #1: Collecting pandas==0.24.2\n",
      "Step #1:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #1: Collecting kfp==0.2.5\n",
      "Step #1:   Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Step #1: Collecting termcolor\n",
      "Step #1:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.4.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2019.3)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #1: Collecting urllib3<1.25,>=1.15\n",
      "Step #1:   Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2019.11.28)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.26.0)\n",
      "Step #1: Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "Step #1:   Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.8)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.11.2)\n",
      "Step #1: Collecting requests_toolbelt>=0.8.0\n",
      "Step #1:   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #1: Collecting cloudpickle==1.1.1\n",
      "Step #1:   Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Step #1: Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "Step #1:   Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Step #1: Collecting argo-models==2.2.1a\n",
      "Step #1:   Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Step #1: Collecting tabulate==0.8.3\n",
      "Step #1:   Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: click==7.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (7.0)\n",
      "Step #1: Collecting Deprecated\n",
      "Step #1:   Downloading Deprecated-1.2.7-py2.py3-none-any.whl (8.3 kB)\n",
      "Step #1: Collecting strip-hints\n",
      "Step #1:   Downloading strip-hints-0.1.8.tar.gz (27 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.23.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (45.2.0.post20200209)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.13.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (3.1.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.15.7)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.5.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.12.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.9)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.19)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.0.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.11.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Step #1: Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "Step #1:   Building wheel for fire (setup.py): started\n",
      "Step #1:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for fire: filename=fire-0.3.0-py2.py3-none-any.whl size=111108 sha256=172945f258617f32b5054706d8402e36349b11cb84a3c754c523a98a149bc04d\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/2c/91/7b/2954fc66c99d47e663eac5d22a6b5a9bf39a230fa80e6782e8\n",
      "Step #1:   Building wheel for kfp (setup.py): started\n",
      "Step #1:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=f3831125c54f39c402bca9908d2500301810946a181171c14c00ea0673cb89fd\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "Step #1:   Building wheel for termcolor (setup.py): started\n",
      "Step #1:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=5f8e591f1226b11b4fe5bbd4637f5ac403e7fcb614f449a26c81b085606cc1ad\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=45418b40e58e2e17c86430d6cbdd6d06c700bd9f67799ea933323f2b93bbfd47\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "Step #1:   Building wheel for argo-models (setup.py): started\n",
      "Step #1:   Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=9abbb4238e18ae7c950a9b6876f6ea218f955c6c286c15ec404d6d49cbe1b158\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "Step #1:   Building wheel for tabulate (setup.py): started\n",
      "Step #1:   Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=6eb8ccbb56cfcc3e669b5c949d11a87fd656d364a79204548d4851f56c2e4b9f\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "Step #1:   Building wheel for strip-hints (setup.py): started\n",
      "Step #1:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for strip-hints: filename=strip_hints-0.1.8-py2.py3-none-any.whl size=19616 sha256=6fc2db2ac65f72ce2b3fe49e722f0ccbe12008ada54f5cb729765aa1cdb5cc0e\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/45/d9/0c/4a8bfd636665a0969edf94d03397e7b01b23bd2129f7b09d4b\n",
      "Step #1: Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "Step #1: \u001b[91mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\n",
      "Step #1: \u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, Deprecated, strip-hints, kfp\n",
      "Step #1:   Attempting uninstall: scikit-learn\n",
      "Step #1:     Found existing installation: scikit-learn 0.22.1\n",
      "Step #1:     Uninstalling scikit-learn-0.22.1:\n",
      "Step #1:       Successfully uninstalled scikit-learn-0.22.1\n",
      "Step #1:   Attempting uninstall: pandas\n",
      "Step #1:     Found existing installation: pandas 1.0.1\n",
      "Step #1:     Uninstalling pandas-1.0.1:\n",
      "Step #1:       Successfully uninstalled pandas-1.0.1\n",
      "Step #1:   Attempting uninstall: urllib3\n",
      "Step #1:     Found existing installation: urllib3 1.25.7\n",
      "Step #1:     Uninstalling urllib3-1.25.7:\n",
      "Step #1:       Successfully uninstalled urllib3-1.25.7\n",
      "Step #1:   Attempting uninstall: kubernetes\n",
      "Step #1:     Found existing installation: kubernetes 10.0.1\n",
      "Step #1:     Uninstalling kubernetes-10.0.1:\n",
      "Step #1:       Successfully uninstalled kubernetes-10.0.1\n",
      "Step #1:   Attempting uninstall: cloudpickle\n",
      "Step #1:     Found existing installation: cloudpickle 1.3.0\n",
      "Step #1:     Uninstalling cloudpickle-1.3.0:\n",
      "Step #1:       Successfully uninstalled cloudpickle-1.3.0\n",
      "Step #1: Successfully installed Deprecated-1.2.7 argo-models-2.2.1a0 cloudpickle-1.1.1 fire-0.3.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.8 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Step #1: Removing intermediate container 31ea7c1e0dbe\n",
      "Step #1:  ---> 282c68c2fb26\n",
      "Step #1: Successfully built 282c68c2fb26\n",
      "Step #1: Successfully tagged gcr.io/mlops-workshop/base_image:test\n",
      "Finished Step #1\n",
      "Starting Step #2\n",
      "Step #2: Pulling image: gcr.io/mlops-workshop/kfp-cli\n",
      "Step #2: Using default tag: latest\n",
      "Step #2: latest: Pulling from mlops-workshop/kfp-cli\n",
      "Step #2: 423ae2b273f4: Already exists\n",
      "Step #2: de83a2304fa1: Already exists\n",
      "Step #2: f9a83bce3af0: Already exists\n",
      "Step #2: b6b53be908de: Already exists\n",
      "Step #2: 5650063cfbfb: Already exists\n",
      "Step #2: 89142850430d: Already exists\n",
      "Step #2: 498b10157bcd: Already exists\n",
      "Step #2: a77a3b1caf74: Already exists\n",
      "Step #2: 0603289dda03: Already exists\n",
      "Step #2: c3ae245b40c1: Already exists\n",
      "Step #2: 67e85692af8b: Already exists\n",
      "Step #2: ea72ab3b7167: Already exists\n",
      "Step #2: b02850f0d90c: Already exists\n",
      "Step #2: 4295de6959ce: Already exists\n",
      "Step #2: d651a7c122d6: Already exists\n",
      "Step #2: 69e0b993e5f5: Already exists\n",
      "Step #2: 448df84a6bd8: Pulling fs layer\n",
      "Step #2: 448df84a6bd8: Verifying Checksum\n",
      "Step #2: 448df84a6bd8: Download complete\n",
      "Step #2: 448df84a6bd8: Pull complete\n",
      "Step #2: Digest: sha256:4c5df46ade8d83b6726efb14639099f5dc6422be05748d0db12a7263eca6f1a7\n",
      "Step #2: Status: Downloaded newer image for gcr.io/mlops-workshop/kfp-cli:latest\n",
      "Step #2: gcr.io/mlops-workshop/kfp-cli:latest\n",
      "Finished Step #2\n",
      "Starting Step #3\n",
      "Step #3: Already have image (with digest): gcr.io/mlops-workshop/kfp-cli\n",
      "Step #3: Pipeline ba74bcd0-b1dc-4f9a-934e-b9814a6707dc has been submitted\n",
      "Step #3: \n",
      "Step #3: Pipeline Details\n",
      "Step #3: ------------------\n",
      "Step #3: ID           ba74bcd0-b1dc-4f9a-934e-b9814a6707dc\n",
      "Step #3: Name         covertype_training_deployment_test\n",
      "Step #3: Description\n",
      "Step #3: Uploaded at  2020-03-25T21:06:24+00:00\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | Parameter Name              | Default Value                                    |\n",
      "Step #3: +=============================+==================================================+\n",
      "Step #3: | project_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | region                      |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | source_table_name           |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | gcs_root                    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | dataset_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | evaluation_metric_name      |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | evaluation_metric_threshold |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | model_id                    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | version_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | replace_existing_version    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | hypertune_settings          | {                                                |\n",
      "Step #3: |                             |     \"hyperparameters\":  {                        |\n",
      "Step #3: |                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "Step #3: |                             |         \"maxTrials\": 6,                          |\n",
      "Step #3: |                             |         \"maxParallelTrials\": 3,                  |\n",
      "Step #3: |                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "Step #3: |                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "Step #3: |                             |         \"params\": [                              |\n",
      "Step #3: |                             |             {                                    |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_iter\",     |\n",
      "Step #3: |                             |                 \"type\": \"DISCRETE\",              |\n",
      "Step #3: |                             |                 \"discreteValues\": [500, 1000]    |\n",
      "Step #3: |                             |             },                                   |\n",
      "Step #3: |                             |             {                                    |\n",
      "Step #3: |                             |                 \"parameterName\": \"alpha\",        |\n",
      "Step #3: |                             |                 \"type\": \"DOUBLE\",                |\n",
      "Step #3: |                             |                 \"minValue\": 0.0001,              |\n",
      "Step #3: |                             |                 \"maxValue\": 0.001,               |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "Step #3: |                             |             }                                    |\n",
      "Step #3: |                             |         ]                                        |\n",
      "Step #3: |                             |     }                                            |\n",
      "Step #3: |                             | }                                                |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | dataset_location            | US                                               |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Finished Step #3\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/trainer_image:test\n",
      "The push refers to repository [gcr.io/mlops-workshop/trainer_image]\n",
      "f442d5ed7db1: Preparing\n",
      "634e3787fca9: Preparing\n",
      "8f40b3d6810f: Preparing\n",
      "5fa62df9720d: Preparing\n",
      "e03d08bfe16a: Preparing\n",
      "f29926cdae57: Preparing\n",
      "78b37de332d9: Preparing\n",
      "3588945eb156: Preparing\n",
      "be3a0415bbb5: Preparing\n",
      "e58eaed7285c: Preparing\n",
      "08054ca51826: Preparing\n",
      "13090698bef4: Preparing\n",
      "b1541e772a6a: Preparing\n",
      "3e964e3b0508: Preparing\n",
      "7a48b46e6485: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "f29926cdae57: Waiting\n",
      "78b37de332d9: Waiting\n",
      "3588945eb156: Waiting\n",
      "be3a0415bbb5: Waiting\n",
      "e58eaed7285c: Waiting\n",
      "08054ca51826: Waiting\n",
      "13090698bef4: Waiting\n",
      "b1541e772a6a: Waiting\n",
      "3e964e3b0508: Waiting\n",
      "7a48b46e6485: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "e03d08bfe16a: Layer already exists\n",
      "5fa62df9720d: Layer already exists\n",
      "f29926cdae57: Layer already exists\n",
      "78b37de332d9: Layer already exists\n",
      "be3a0415bbb5: Layer already exists\n",
      "3588945eb156: Layer already exists\n",
      "e58eaed7285c: Layer already exists\n",
      "08054ca51826: Layer already exists\n",
      "13090698bef4: Layer already exists\n",
      "b1541e772a6a: Layer already exists\n",
      "634e3787fca9: Pushed\n",
      "f442d5ed7db1: Pushed\n",
      "1852b2300972: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "7a48b46e6485: Layer already exists\n",
      "3e964e3b0508: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "8f40b3d6810f: Pushed\n",
      "test: digest: sha256:0c7381dff7d79644a6a7fed840bfeb83e44a06e6ffd11fb334e107d4e098c552 size: 4292\n",
      "Pushing gcr.io/mlops-workshop/base_image:test\n",
      "The push refers to repository [gcr.io/mlops-workshop/base_image]\n",
      "d98feff2ce40: Preparing\n",
      "5fa62df9720d: Preparing\n",
      "e03d08bfe16a: Preparing\n",
      "f29926cdae57: Preparing\n",
      "78b37de332d9: Preparing\n",
      "3588945eb156: Preparing\n",
      "be3a0415bbb5: Preparing\n",
      "e58eaed7285c: Preparing\n",
      "08054ca51826: Preparing\n",
      "13090698bef4: Preparing\n",
      "b1541e772a6a: Preparing\n",
      "3e964e3b0508: Preparing\n",
      "7a48b46e6485: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "3588945eb156: Waiting\n",
      "be3a0415bbb5: Waiting\n",
      "e58eaed7285c: Waiting\n",
      "08054ca51826: Waiting\n",
      "13090698bef4: Waiting\n",
      "b1541e772a6a: Waiting\n",
      "3e964e3b0508: Waiting\n",
      "7a48b46e6485: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "5fa62df9720d: Layer already exists\n",
      "e03d08bfe16a: Layer already exists\n",
      "f29926cdae57: Layer already exists\n",
      "78b37de332d9: Layer already exists\n",
      "3588945eb156: Layer already exists\n",
      "be3a0415bbb5: Layer already exists\n",
      "08054ca51826: Layer already exists\n",
      "e58eaed7285c: Layer already exists\n",
      "13090698bef4: Layer already exists\n",
      "b1541e772a6a: Layer already exists\n",
      "7a48b46e6485: Layer already exists\n",
      "3e964e3b0508: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "d98feff2ce40: Pushed\n",
      "test: digest: sha256:b752e229ea2479c156baccd561d30285a6ed5c5628d2c06345570682f77e4fe7 size: 3876\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                              STATUS\n",
      "2c0071fb-65f8-4c51-ba77-3979861853b9  2020-03-25T21:02:24+00:00  4M22S     gs://mlops-workshop_cloudbuild/source/1585170143.86-e219a711d25c4d669e72054429bdfbdd.tgz  gcr.io/mlops-workshop/trainer_image:test (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit .. --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location|/ workshops/kfp-caip-sklearn/lab-03-kfp-cicd/cloudbuild.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_BASE_IMAGE_NAME|base_image|\n",
    "|_COMPONENT_URL_SEARCH_PREFIX|https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/|\n",
    "|_ENDPOINT|[Your inverting proxy host]|\n",
    "|_PIPELINE_DSL|covertype_training_pipeline.py|\n",
    "|_PIPELINE_FOLDER|workshops/kfp-caip-sklearn/lab-02-kfp-pipeline|\n",
    "|_PIPELINE_NAME|covertype_training_deployment|\n",
    "|_PIPELINE_PACKAGE|covertype_training_pipeline.yaml|\n",
    "|_PYTHON_VERSION|3.7|\n",
    "|_RUNTIME_VERSION|1.15|\n",
    "|_TRAINER_IMAGE_NAME|trainer_image|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
