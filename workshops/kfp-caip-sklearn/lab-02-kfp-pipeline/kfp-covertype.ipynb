{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training pipeline with KFP and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will review, deploy, and run a KFP pipeline that orchestrates **BigQuery** and **Cloud AI Platform** services to train a **scikit-learn** model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n",
    "The pipeline source code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxr-xr-x 2 root root 4096 Mar 12 03:45 .\n",
      "drwxr-xr-x 6 root root 4096 Mar 12 03:45 ..\n",
      "-rw-r--r-- 1 root root 7260 Mar 12 03:34 covertype_training_pipeline.py\n",
      "-rw-r--r-- 1 root root 2846 Mar 12 03:14 helper_components.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based KFP Domain Specific Language (DSL). The pipeline's DSL is in the `covertype_training_pipeline.py` file. \n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
      "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
      "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
      "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
      "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!grep 'BASE_IMAGE =' -A 5 pipeline/covertype_training_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline uses a mix of custom and pre-build components.\n",
    "\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with the KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.2.4/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.2.4/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.2.4/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates the *sklearn* trained model using a provided metric and a testing dataset.\n",
    "    \n",
    "The custom components execute in a container image defined in `base_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step in the pipeline employes the AI Platform Training component to schedule a  AI Platform Training job in a custom training container. The custom training image is defined in `trainer_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "Before deploying to AI Platform Pipelines, the pipeline DSL has to be compiled into a pipeline runtime format, also refered to as a pipeline package.  The runtime format is based on [Argo Workflow](https://github.com/argoproj/argo), which is expressed in YAML. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '11980e931804006b-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://hostedkfp-default-1yu1vvsv25'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/mlops-dev-100/trainer_image:latest'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)\n",
    "\n",
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 3.6 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://mlops-dev-100_cloudbuild/source/1583986299.41-d5db5947a0ee408889678c33db2eeea0.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-dev-100/builds/85201496-a5cc-48e8-aee4-9be69eb20349].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/85201496-a5cc-48e8-aee4-9be69eb20349?project=286479790129].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"85201496-a5cc-48e8-aee4-9be69eb20349\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-dev-100_cloudbuild/source/1583986299.41-d5db5947a0ee408889678c33db2eeea0.tgz#1583986299674523\n",
      "Copying gs://mlops-dev-100_cloudbuild/source/1583986299.41-d5db5947a0ee408889678c33db2eeea0.tgz#1583986299674523...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "423ae2b273f4: Pulling fs layer\n",
      "de83a2304fa1: Pulling fs layer\n",
      "f9a83bce3af0: Pulling fs layer\n",
      "b6b53be908de: Pulling fs layer\n",
      "5650063cfbfb: Pulling fs layer\n",
      "89142850430d: Pulling fs layer\n",
      "498b10157bcd: Pulling fs layer\n",
      "a77a3b1caf74: Pulling fs layer\n",
      "0603289dda03: Pulling fs layer\n",
      "c3ae245b40c1: Pulling fs layer\n",
      "67e85692af8b: Pulling fs layer\n",
      "ea72ab3b7167: Pulling fs layer\n",
      "b02850f0d90c: Pulling fs layer\n",
      "4295de6959ce: Pulling fs layer\n",
      "d651a7c122d6: Pulling fs layer\n",
      "69e0b993e5f5: Pulling fs layer\n",
      "b6b53be908de: Waiting\n",
      "5650063cfbfb: Waiting\n",
      "89142850430d: Waiting\n",
      "498b10157bcd: Waiting\n",
      "a77a3b1caf74: Waiting\n",
      "0603289dda03: Waiting\n",
      "c3ae245b40c1: Waiting\n",
      "67e85692af8b: Waiting\n",
      "ea72ab3b7167: Waiting\n",
      "b02850f0d90c: Waiting\n",
      "4295de6959ce: Waiting\n",
      "d651a7c122d6: Waiting\n",
      "69e0b993e5f5: Waiting\n",
      "f9a83bce3af0: Verifying Checksum\n",
      "f9a83bce3af0: Download complete\n",
      "de83a2304fa1: Verifying Checksum\n",
      "de83a2304fa1: Download complete\n",
      "423ae2b273f4: Verifying Checksum\n",
      "423ae2b273f4: Download complete\n",
      "b6b53be908de: Verifying Checksum\n",
      "b6b53be908de: Download complete\n",
      "498b10157bcd: Verifying Checksum\n",
      "498b10157bcd: Download complete\n",
      "89142850430d: Verifying Checksum\n",
      "89142850430d: Download complete\n",
      "a77a3b1caf74: Verifying Checksum\n",
      "a77a3b1caf74: Download complete\n",
      "c3ae245b40c1: Verifying Checksum\n",
      "c3ae245b40c1: Download complete\n",
      "67e85692af8b: Verifying Checksum\n",
      "67e85692af8b: Download complete\n",
      "ea72ab3b7167: Verifying Checksum\n",
      "ea72ab3b7167: Download complete\n",
      "b02850f0d90c: Verifying Checksum\n",
      "b02850f0d90c: Download complete\n",
      "4295de6959ce: Verifying Checksum\n",
      "4295de6959ce: Download complete\n",
      "5650063cfbfb: Verifying Checksum\n",
      "5650063cfbfb: Download complete\n",
      "d651a7c122d6: Verifying Checksum\n",
      "d651a7c122d6: Download complete\n",
      "69e0b993e5f5: Verifying Checksum\n",
      "69e0b993e5f5: Download complete\n",
      "423ae2b273f4: Pull complete\n",
      "0603289dda03: Verifying Checksum\n",
      "0603289dda03: Download complete\n",
      "de83a2304fa1: Pull complete\n",
      "f9a83bce3af0: Pull complete\n",
      "b6b53be908de: Pull complete\n",
      "5650063cfbfb: Pull complete\n",
      "89142850430d: Pull complete\n",
      "498b10157bcd: Pull complete\n",
      "a77a3b1caf74: Pull complete\n",
      "0603289dda03: Pull complete\n",
      "c3ae245b40c1: Pull complete\n",
      "67e85692af8b: Pull complete\n",
      "ea72ab3b7167: Pull complete\n",
      "b02850f0d90c: Pull complete\n",
      "4295de6959ce: Pull complete\n",
      "d651a7c122d6: Pull complete\n",
      "69e0b993e5f5: Pull complete\n",
      "Digest: sha256:d052cdcfacd704995a3a54650f43aeba5228228a66af67833304b6e8a18060f1\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b9a9911547cc\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 1f85a4926359\n",
      "Collecting fire\n",
      "  Downloading fire-0.2.1.tar.gz (76 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2019.3)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=91f2cb68658439dfd3743940d2198d50a38f0fba7c93d171d8359969757a607d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/6d/a8/d81d42414b24203fc8beb0452deab949ba62fcfb8c7a49e4b6\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=0696714c5dbe9ec6d8ed94acd76f623ecd1e31bda97d7b44ec396cd56c1f9368\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c70074fa44f043c5199d90d02b6f15ad30ff85b194e21ca5e859ad1ba06e2814\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.2.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 1f85a4926359\n",
      " ---> c5685c013340\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in e5ca49b9e1ae\n",
      "Removing intermediate container e5ca49b9e1ae\n",
      " ---> 65bb926b5a21\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> c872093722d8\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 09d692adbec1\n",
      "Removing intermediate container 09d692adbec1\n",
      " ---> 3ccf533f8339\n",
      "Successfully built 3ccf533f8339\n",
      "Successfully tagged gcr.io/mlops-dev-100/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-dev-100/trainer_image:latest\n",
      "The push refers to repository [gcr.io/mlops-dev-100/trainer_image]\n",
      "8b9995ec74ac: Preparing\n",
      "b22864648a37: Preparing\n",
      "d07ea95e42ce: Preparing\n",
      "5fa62df9720d: Preparing\n",
      "e03d08bfe16a: Preparing\n",
      "f29926cdae57: Preparing\n",
      "78b37de332d9: Preparing\n",
      "3588945eb156: Preparing\n",
      "be3a0415bbb5: Preparing\n",
      "e58eaed7285c: Preparing\n",
      "08054ca51826: Preparing\n",
      "13090698bef4: Preparing\n",
      "b1541e772a6a: Preparing\n",
      "3e964e3b0508: Preparing\n",
      "7a48b46e6485: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "f29926cdae57: Waiting\n",
      "78b37de332d9: Waiting\n",
      "3588945eb156: Waiting\n",
      "be3a0415bbb5: Waiting\n",
      "e58eaed7285c: Waiting\n",
      "08054ca51826: Waiting\n",
      "13090698bef4: Waiting\n",
      "b1541e772a6a: Waiting\n",
      "3e964e3b0508: Waiting\n",
      "7a48b46e6485: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "5fa62df9720d: Layer already exists\n",
      "e03d08bfe16a: Layer already exists\n",
      "78b37de332d9: Layer already exists\n",
      "f29926cdae57: Layer already exists\n",
      "3588945eb156: Layer already exists\n",
      "be3a0415bbb5: Layer already exists\n",
      "08054ca51826: Layer already exists\n",
      "e58eaed7285c: Layer already exists\n",
      "b22864648a37: Pushed\n",
      "13090698bef4: Layer already exists\n",
      "8b9995ec74ac: Pushed\n",
      "b1541e772a6a: Layer already exists\n",
      "3e964e3b0508: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "7a48b46e6485: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "d07ea95e42ce: Pushed\n",
      "latest: digest: sha256:1374cc01f07108be803ca58206aca1e270f4936c8e905dceae75b5e23168ef54 size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                        STATUS\n",
      "85201496-a5cc-48e8-aee4-9be69eb20349  2020-03-12T04:11:39+00:00  3M45S     gs://mlops-dev-100_cloudbuild/source/1583986299.41-d5db5947a0ee408889678c33db2eeea0.tgz  gcr.io/mlops-dev-100/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the base image for custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE=TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 244 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://mlops-dev-100_cloudbuild/source/1583986788.97-a2088edcb59947c1afe93dbc399388c3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-dev-100/builds/1789cc50-88c4-4f8a-b422-88e558cba5f6].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1789cc50-88c4-4f8a-b422-88e558cba5f6?project=286479790129].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1789cc50-88c4-4f8a-b422-88e558cba5f6\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-dev-100_cloudbuild/source/1583986788.97-a2088edcb59947c1afe93dbc399388c3.tgz#1583986789269658\n",
      "Copying gs://mlops-dev-100_cloudbuild/source/1583986788.97-a2088edcb59947c1afe93dbc399388c3.tgz#1583986789269658...\n",
      "/ [1 files][  289.0 B/  289.0 B]                                                \n",
      "Operation completed over 1 objects/289.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "423ae2b273f4: Pulling fs layer\n",
      "de83a2304fa1: Pulling fs layer\n",
      "f9a83bce3af0: Pulling fs layer\n",
      "b6b53be908de: Pulling fs layer\n",
      "5650063cfbfb: Pulling fs layer\n",
      "89142850430d: Pulling fs layer\n",
      "498b10157bcd: Pulling fs layer\n",
      "a77a3b1caf74: Pulling fs layer\n",
      "0603289dda03: Pulling fs layer\n",
      "c3ae245b40c1: Pulling fs layer\n",
      "67e85692af8b: Pulling fs layer\n",
      "ea72ab3b7167: Pulling fs layer\n",
      "b02850f0d90c: Pulling fs layer\n",
      "4295de6959ce: Pulling fs layer\n",
      "d651a7c122d6: Pulling fs layer\n",
      "69e0b993e5f5: Pulling fs layer\n",
      "b6b53be908de: Waiting\n",
      "5650063cfbfb: Waiting\n",
      "89142850430d: Waiting\n",
      "498b10157bcd: Waiting\n",
      "a77a3b1caf74: Waiting\n",
      "0603289dda03: Waiting\n",
      "c3ae245b40c1: Waiting\n",
      "67e85692af8b: Waiting\n",
      "ea72ab3b7167: Waiting\n",
      "b02850f0d90c: Waiting\n",
      "4295de6959ce: Waiting\n",
      "d651a7c122d6: Waiting\n",
      "69e0b993e5f5: Waiting\n",
      "f9a83bce3af0: Verifying Checksum\n",
      "f9a83bce3af0: Download complete\n",
      "de83a2304fa1: Verifying Checksum\n",
      "de83a2304fa1: Download complete\n",
      "423ae2b273f4: Verifying Checksum\n",
      "423ae2b273f4: Download complete\n",
      "b6b53be908de: Verifying Checksum\n",
      "b6b53be908de: Download complete\n",
      "498b10157bcd: Verifying Checksum\n",
      "498b10157bcd: Download complete\n",
      "89142850430d: Verifying Checksum\n",
      "89142850430d: Download complete\n",
      "a77a3b1caf74: Verifying Checksum\n",
      "a77a3b1caf74: Download complete\n",
      "c3ae245b40c1: Verifying Checksum\n",
      "c3ae245b40c1: Download complete\n",
      "67e85692af8b: Verifying Checksum\n",
      "67e85692af8b: Download complete\n",
      "ea72ab3b7167: Verifying Checksum\n",
      "ea72ab3b7167: Download complete\n",
      "b02850f0d90c: Verifying Checksum\n",
      "b02850f0d90c: Download complete\n",
      "4295de6959ce: Verifying Checksum\n",
      "4295de6959ce: Download complete\n",
      "d651a7c122d6: Verifying Checksum\n",
      "d651a7c122d6: Download complete\n",
      "5650063cfbfb: Verifying Checksum\n",
      "5650063cfbfb: Download complete\n",
      "69e0b993e5f5: Verifying Checksum\n",
      "69e0b993e5f5: Download complete\n",
      "423ae2b273f4: Pull complete\n",
      "0603289dda03: Verifying Checksum\n",
      "0603289dda03: Download complete\n",
      "de83a2304fa1: Pull complete\n",
      "f9a83bce3af0: Pull complete\n",
      "b6b53be908de: Pull complete\n",
      "5650063cfbfb: Pull complete\n",
      "89142850430d: Pull complete\n",
      "498b10157bcd: Pull complete\n",
      "a77a3b1caf74: Pull complete\n",
      "0603289dda03: Pull complete\n",
      "c3ae245b40c1: Pull complete\n",
      "67e85692af8b: Pull complete\n",
      "ea72ab3b7167: Pull complete\n",
      "b02850f0d90c: Pull complete\n",
      "4295de6959ce: Pull complete\n",
      "d651a7c122d6: Pull complete\n",
      "69e0b993e5f5: Pull complete\n",
      "Digest: sha256:d052cdcfacd704995a3a54650f43aeba5228228a66af67833304b6e8a18060f1\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b9a9911547cc\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 19bbc6d815c6\n",
      "Collecting fire\n",
      "  Downloading fire-0.2.1.tar.gz (76 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.26.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.11.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Requirement already satisfied, skipping upgrade: click==7.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (7.0)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.7-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.8.tar.gz (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (45.2.0.post20200209)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.13.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.15.7)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.11.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=42ade5a79709910891cdf5b01136d7123b25977bd41f2da70ef836a146b0a84c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/6d/a8/d81d42414b24203fc8beb0452deab949ba62fcfb8c7a49e4b6\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=9bce2f26665e585784a1bd69cdc3264d50dc8036d3cdbadb10a45355d7bd7e48\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c55e0304f8cd3bcb535ef481692de6d762ec3eeef18edca687ba139458a1c4f4\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=4dc8af53f1de8afab4fbc9c062d08e1b383e32ea688a13cb6e599622cdacdc49\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=0ffffdf6157a8c3d8301100b4ac9ba9ecf4429b9ee577931d640a5c9c7b9e05a\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=16677fda9910a86d5048afd9f9e771108b49e3b85100a32b4a2ea2e7da24c459\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.8-py2.py3-none-any.whl size=19616 sha256=293c9d84f13244e6ecb32bac534240c3f39c24746c70d20c0f52c9d1f55d52e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/d9/0c/4a8bfd636665a0969edf94d03397e7b01b23bd2129f7b09d4b\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.4.0 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.7\n",
      "    Uninstalling urllib3-1.25.7:\n",
      "      Successfully uninstalled urllib3-1.25.7\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 10.0.1\n",
      "    Uninstalling kubernetes-10.0.1:\n",
      "      Successfully uninstalled kubernetes-10.0.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "Successfully installed Deprecated-1.2.7 argo-models-2.2.1a0 cloudpickle-1.1.1 fire-0.2.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.8 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 19bbc6d815c6\n",
      " ---> 97c28069d9c1\n",
      "Successfully built 97c28069d9c1\n",
      "Successfully tagged gcr.io/mlops-dev-100/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-dev-100/base_image:latest\n",
      "The push refers to repository [gcr.io/mlops-dev-100/base_image]\n",
      "17156dfef91c: Preparing\n",
      "5fa62df9720d: Preparing\n",
      "e03d08bfe16a: Preparing\n",
      "f29926cdae57: Preparing\n",
      "78b37de332d9: Preparing\n",
      "3588945eb156: Preparing\n",
      "be3a0415bbb5: Preparing\n",
      "e58eaed7285c: Preparing\n",
      "08054ca51826: Preparing\n",
      "13090698bef4: Preparing\n",
      "b1541e772a6a: Preparing\n",
      "3e964e3b0508: Preparing\n",
      "7a48b46e6485: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "3588945eb156: Waiting\n",
      "be3a0415bbb5: Waiting\n",
      "e58eaed7285c: Waiting\n",
      "08054ca51826: Waiting\n",
      "13090698bef4: Waiting\n",
      "b1541e772a6a: Waiting\n",
      "3e964e3b0508: Waiting\n",
      "7a48b46e6485: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "78b37de332d9: Layer already exists\n",
      "f29926cdae57: Layer already exists\n",
      "e03d08bfe16a: Layer already exists\n",
      "5fa62df9720d: Layer already exists\n",
      "be3a0415bbb5: Layer already exists\n",
      "e58eaed7285c: Layer already exists\n",
      "3588945eb156: Layer already exists\n",
      "08054ca51826: Layer already exists\n",
      "3e964e3b0508: Layer already exists\n",
      "13090698bef4: Layer already exists\n",
      "b1541e772a6a: Layer already exists\n",
      "7a48b46e6485: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "17156dfef91c: Pushed\n",
      "latest: digest: sha256:af0338b0a9a0987c50d4d6120f4e855a2412670481e691ebe77321bdc120e835 size: 3876\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                     STATUS\n",
      "1789cc50-88c4-4f8a-b422-88e558cba5f6  2020-03-12T04:19:49+00:00  3M54S     gs://mlops-dev-100_cloudbuild/source/1583986788.97-a2088edcb59947c1afe93dbc399388c3.tgz  gcr.io/mlops-dev-100/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can compile the DSL using an API from the **KFP SDK** or using the **KFP** compiler.\n",
    "\n",
    "To compile the pipeline DSL using **KFP** compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_IMAGE=gcr.io/mlops-dev-100/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/mlops-dev-100/base_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=2.1\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '2.1'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the `covertype_training_pipeline.yaml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"apiVersion\": |-\n",
      "  argoproj.io/v1alpha1\n",
      "\"kind\": |-\n",
      "  Workflow\n",
      "\"metadata\":\n",
      "  \"annotations\":\n",
      "    \"pipelines.kubeflow.org/pipeline_spec\": |-\n",
      "      {\"description\": \"The pipeline training and deploying the Covertype classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"}, {\"name\": \"region\"}, {\"name\": \"source_table_name\"}, {\"name\": \"gcs_root\"}, {\"name\": \"dataset_id\"}, {\"name\": \"evaluation_metric_name\"}, {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"}, {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\": \\\"MAXIMIZE\\\",\\n        \\\"maxTrials\\\": 6,\\n        \\\"maxParallelTrials\\\": 3,\\n        \\\"hyperparameterMetricTag\\\": \\\"accuracy\\\",\\n        \\\"enableTrialEarlyStopping\\\": True,\\n        \\\"params\\\": [\\n            {\\n                \\\"parameterName\\\": \\\"max_iter\\\",\\n                \\\"type\\\": \\\"DISCRETE\\\",\\n                \\\"discreteValues\\\": [500, 1000]\\n            },\\n            {\\n                \\\"parameterName\\\": \\\"alpha\\\",\\n                \\\"type\\\": \\\"DOUBLE\\\",\\n                \\\"minValue\\\": 0.0001,\\n                \\\"maxValue\\\": 0.001,\\n                \\\"scaleType\\\": \\\"UNIT_LINEAR_SCALE\\\"\\n            }\\n        ]\\n    }\\n}\\n\", \"name\": \"hypertune_settings\", \"optional\": true}, {\"default\": \"US\", \"name\": \"dataset_location\", \"optional\": true}], \"name\": \"Covertype Classifier Training\"}\n",
      "  \"generateName\": |-\n",
      "    covertype-classifier-training-\n"
     ]
    }
   ],
   "source": [
    "!head covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cd5d6afe-fcdf-4df6-b879-9efd430456a7 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           cd5d6afe-fcdf-4df6-b879-9efd430456a7\n",
      "Name         covertype_classifier_training\n",
      "Description\n",
      "Uploaded at  2020-03-12T04:51:15+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_classifier_training'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting pipeline runs\n",
    "\n",
    "You can trigger pipeline runs using an API from the KFP SDK or using KFP CLI. To submit the run using KFP CLI, execute the following commands. Notice how the pipeline's parameters are passed to the pipeline run.\n",
    "\n",
    "### List the pipelines in AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| cd5d6afe-fcdf-4df6-b879-9efd430456a7 | covertype_classifier_training                   | 2020-03-12T04:51:15+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b29a6dcf-4dda-4320-881d-9d8b3a762211 | [Tutorial] DSL - Control structures             | 2020-03-12T02:31:33+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 18691881-cba5-413b-907f-59b6a9f8d901 | [Tutorial] Data passing in python components    | 2020-03-12T02:31:32+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b36d01b6-ec87-43d9-9fc9-cc8c2c62fc11 | [Demo] TFX - Taxi Tip Prediction Model Trainer  | 2020-03-12T02:31:31+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 55477d21-dd09-43fc-a914-bf5887a5e7e5 | [Demo] XGBoost - Training with Confusion Matrix | 2020-03-12T02:31:30+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the ID of the `covertype_classifier_training` pipeline you uploaded in the previous step and update the value of `PIPELINE_ID` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='cd5d6afe-fcdf-4df6-b879-9efd430456a7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment Covertype_Classifier_Training.\n",
      "Run 3f2f73e1-d7ac-4618-a006-ee8e244dbfba is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 3f2f73e1-d7ac-4618-a006-ee8e244dbfba | Run_001 |          | 2020-03-12T05:05:15+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- EXPERIMENT_NAME is set to the experiment used to run the pipeline. You can choose any name you want. If the experiment does not exist it will be created by the command\n",
    "- RUN_ID is the name of the run. You can use an arbitrary name\n",
    "- PIPELINE_ID is the id of your pipeline. Use the value retrieved by the   `kfp pipeline list` command\n",
    "- GCS_STAGING_PATH is the URI to the GCS location used by the pipeline to store intermediate files. By default, it is set to the `staging` folder in your artifact store.\n",
    "- REGION is a compute region for AI Platform Training and Prediction. \n",
    "\n",
    "You should be already familiar with these and other parameters passed to the command. If not go back and review the pipeline code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the run\n",
    "\n",
    "You can monitor the run using KFP UI. Follow the instructor who will walk you through the KFP UI and monitoring techniques.\n",
    "\n",
    "To access the KFP UI in your environment use the following URI:\n",
    "\n",
    "https://[ENDPOINT]\n",
    "\n",
    "\n",
    "*Note that your pipeline may fail due to the bug in a BigQuery component that does not handle certain race conditions. If you observe the pipeline failure retry the run from the KFP UI*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
